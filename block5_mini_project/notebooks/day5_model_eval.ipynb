{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb717c78",
   "metadata": {},
   "source": [
    "# Day 5: Model Evaluation and Comparison\n",
    "\n",
    "## 1. Objective\n",
    "\n",
    "Evaluate and compare the performance of two machine learning models—**Logistic Regression** and **Random Forest**—trained on a gallstone risk dataset. Focus on real-world decision-making factors such as error trade-offs, explainability, and clinical relevance.\n",
    "\n",
    "## 2. Key Steps\n",
    "\n",
    "- Reload cleaned dataset and trained models (`.joblib`)\n",
    "- Restore consistent data types and train/test split\n",
    "- Rescale test data for logistic regression using saved `StandardScaler`\n",
    "- Generate predictions and probability scores on the test set\n",
    "- Compute classification metrics: Accuracy, Precision, Recall, F1, ROC AUC\n",
    "- Validate generalization with 5-fold cross-validation AUC scores\n",
    "- Visualize results:\n",
    "  - Confusion Matrices\n",
    "  - ROC Curves with inline AUC labels\n",
    "  - Summary metric comparison table\n",
    "- Reflect on error types (false positives vs. false negatives)\n",
    "- Interpret clinical significance of top model features\n",
    "- Apply **Storytelling with Data** principles to improve clarity and user trust\n",
    "\n",
    "## 3. Results\n",
    "\n",
    "- **Logistic Regression** achieved the best test performance with an ROC AUC of 0.811 and higher recall (0.719), suggesting stronger reliability in identifying gallstone-positive cases.\n",
    "- **Random Forest** performed well in cross-validation (AUC = 0.841) but slightly underperformed on the test set, indicating possible overfitting or reduced generalizability.\n",
    "- ROC curves, confusion matrices, and a metric table clarified the trade-offs between both models.\n",
    "\n",
    "## 4. Summary\n",
    "\n",
    "While Random Forest offered more complexity and slightly higher CV-AUC, Logistic Regression emerged as the more **robust, interpretable, and clinically reliable model** in this case. Its performance and transparency make it a better candidate for deployment in decision support systems where **minimizing false negatives** and maintaining clinician trust are critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed43b30f",
   "metadata": {},
   "source": [
    "## Load Cleaned Dataset and Trained Models\n",
    "\n",
    "We begin by reloading the cleaned dataset and reapplying test/train splits as before. Then we reload the previously saved `.joblib` model files for logistic regression and random forest, as well as the scaler used for standardizing the logistic regression input.\n",
    "\n",
    "Each model is evaluated on the same input configuration it was trained with:\n",
    "- **Logistic Regression**: Uses VIF-pruned, standardized features (`X_test_reduced_scaled`)\n",
    "- **Random Forest**: Uses the full test set (`X_test`) and handles scaling internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a9efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv(\"../data/cleaned.csv\")\n",
    "\n",
    "# Restore categorical types\n",
    "categorical_cols = [\n",
    "    # Original binary or ordinal clinical features\n",
    "    \"gender\",\n",
    "    \"comorbidity\",\n",
    "    \"cad\",\n",
    "    \"hypothyroidism\",\n",
    "    \"hyperlipidemia\",\n",
    "    \"diabetes\",\n",
    "    \"hepatic_fat\",\n",
    "    \"has_gallstones\",\n",
    "\n",
    "    # Outlier flags (created during data cleaning)\n",
    "    \"glucose_outlier_flag\",\n",
    "    \"obesity_outlier_flag\",\n",
    "    \"muscle_mass_outlier_flag\",\n",
    "    \"gfr_outlier_flag\",\n",
    "    \"tbw_outlier_flag\",\n",
    "    \"icw_outlier_flag\",\n",
    "    \"vfr_outlier_flag\",\n",
    "    \"ldl_outlier_flag\",\n",
    "    \"hdl_outlier_flag\",\n",
    "    \"triglyceride_outlier_flag\",\n",
    "    \"alt_outlier_flag\",\n",
    "    \"crp_outlier_flag\"\n",
    "]\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# Define target and features\n",
    "target = \"has_gallstones\"\n",
    "y = df[target].replace({1: \"Gallstones\", 0: \"No Gallstones\"})  # Keep 0 and 1 for models\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "# Train-test split (same seed for consistency)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reload dropped features list used for logistic regression VIF reduction\n",
    "features_dropped = ['height_cm', 'weight_kg', 'ecf_tbw_ratio_index', 'muscle_mass_kg',\n",
    "    'lean_mass_percent', 'bmi', 'ecw_kg', 'tbw_kg', 'icw_kg', 'fat_mass_kg', 'visceral_muscle_mass_kg',\n",
    "    'cholesterol_total_mg_dl', 'hemoglobin_g_dl', 'bone_mass_kg', 'protein_percent', 'visceral_fat_area_cm2',\n",
    "    'fat_ratio_percent', 'age', 'gfr_ml_min', 'creatinine_mg_dl', 'ldl_mg_dl', 'vfr_score', 'alp_u_l']\n",
    "\n",
    "# Recreate VIF-reduced test set\n",
    "X_reduced = X.drop(columns=features_dropped)\n",
    "X_train_reduced = X_train.drop(columns=features_dropped)\n",
    "X_test_reduced = X_test.drop(columns=features_dropped)\n",
    "\n",
    "# Load models and scaler\n",
    "logreg = joblib.load(\"../models/logistic_regression_model.joblib\")\n",
    "rf = joblib.load(\"../models/random_forest_model.joblib\")\n",
    "logreg_scaler = joblib.load(\"../models/logreg_scaler.joblib\")\n",
    "\n",
    "# VIF-reduced test set for logistic regression\n",
    "X_scaled = logreg_scaler.transform(X_reduced)\n",
    "X_train_scaled = logreg_scaler.transform(X_train_reduced)\n",
    "X_test_scaled = logreg_scaler.transform(X_test_reduced)\n",
    "\n",
    "# Logistic Regression\n",
    "logreg_preds = logreg.predict(X_test_scaled)  # returns 0 or 1\n",
    "logreg_probs = logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Random Forest\n",
    "rf_preds = rf.predict(X_test)                # returns 0 or 1\n",
    "rf_probs = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Models, scaler, and predictions loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b2fde",
   "metadata": {},
   "source": [
    "## Model and Data Rehydration – Validation Setup\n",
    "\n",
    "In this section, we restored the full modeling pipeline to enable final validation and comparison:\n",
    "\n",
    "- The cleaned dataset was reloaded, and categorical variables were re-cast to their appropriate types to maintain model compatibility.\n",
    "- The original binary target (`has_gallstones`) was preserved as integers (0 = No Gallstones, 1 = Gallstones) for use in downstream classification metrics.\n",
    "- Using the same `random_state`, the dataset was split into training and test sets to ensure consistency with previous model training sessions.\n",
    "- We reloaded the trained **Logistic Regression** and **Random Forest** models, along with the standard scaler used for the VIF-reduced logistic regression features.\n",
    "- The test data was transformed accordingly:\n",
    "  - `X_test_scaled` for logistic regression using the previously saved scaler.\n",
    "  - Full `X_test` for random forest, which had been trained on all features.\n",
    "- Predictions and probability scores were generated for both models using the rehydrated test data.\n",
    "\n",
    "This step ensures a **consistent and reproducible test environment** for evaluating both models under identical test conditions, forming the basis for rigorous comparative performance analysis in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866028e0",
   "metadata": {},
   "source": [
    "## Calculate and Compare Core Classification Metrics\n",
    "\n",
    "In this step, we compute the key performance metrics for both models using the test set:\n",
    "\n",
    "- **Accuracy**: Overall proportion of correct predictions\n",
    "- **Precision**: Proportion of predicted positives that are true positives\n",
    "- **Recall**: Proportion of actual positives correctly identified\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "- **ROC AUC**: Area under the Receiver Operating Characteristic curve; evaluates true positive vs. false positive trade-off\n",
    "\n",
    "These metrics will help assess both model **effectiveness** and **error trade-offs** for clinical decision support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d81621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define function for evaluating metrics\n",
    "def evaluate_model(name, y_true, y_pred, y_prob):\n",
    "    print(f\"{name} Metrics:\")\n",
    "    print(f\"  Accuracy : {accuracy_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"  Precision: {precision_score(y_true, y_pred, pos_label='Gallstones'):.3f}\")\n",
    "    print(f\"  Recall   : {recall_score(y_true, y_pred, pos_label='Gallstones'):.3f}\")\n",
    "    print(f\"  F1 Score : {f1_score(y_true, y_pred, pos_label='Gallstones'):.3f}\")\n",
    "    print(f\"  ROC AUC  : {roc_auc_score((y_true == y_pred[1]).astype(int), y_prob):.3f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate both models\n",
    "evaluate_model(\"Logistic Regression\", y_test, logreg_preds, logreg_probs)\n",
    "evaluate_model(\"Random Forest\", y_test, rf_preds, rf_probs)\n",
    "\n",
    "cv_scores_logreg = cross_val_score(logreg, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "cv_scores_rf = cross_val_score(rf, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"Logistic Regression AUC (CV):\", cv_scores_logreg.mean())\n",
    "print(\"Random Forest AUC (CV):\", cv_scores_rf.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c2a3a3",
   "metadata": {},
   "source": [
    "## Model Performance Summary\n",
    "\n",
    "We evaluated both the logistic regression (baseline) and random forest (advanced) models using two strategies:\n",
    "- **Holdout evaluation on the test set** for an unbiased assessment of performance\n",
    "- **5-fold cross-validation on the training set** to estimate generalization performance\n",
    "\n",
    "### Test Set Performance\n",
    "| Metric        | Logistic Regression | Random Forest |\n",
    "|---------------|---------------------|----------------|\n",
    "| Accuracy      | 0.750               | 0.703          |\n",
    "| Precision     | 0.767               | 0.724          |\n",
    "| Recall        | 0.719               | 0.656          |\n",
    "| F1 Score      | 0.742               | 0.689          |\n",
    "| ROC AUC       | 0.811               | 0.791          |\n",
    "\n",
    "- Logistic Regression **outperforms Random Forest across all test metrics**, including recall and ROC AUC — key measures for identifying gallstone-positive patients.\n",
    "- The performance gap suggests better generalization from the simpler model in this setting.\n",
    "\n",
    "### Cross-Validation AUC (5-Fold on Training Set)\n",
    "| Model                | Mean ROC AUC (CV) |\n",
    "|----------------------|------------------|\n",
    "| Logistic Regression  | 0.814             |\n",
    "| Random Forest        | 0.841             |\n",
    "\n",
    "- **Random Forest shows higher cross-validated AUC**, indicating stronger potential in capturing complex relationships — though this does not fully translate to the test set.\n",
    "- This discrepancy could point to **mild overfitting** or **limited signal in the additional complexity** of the RF model.\n",
    "\n",
    "### Takeaways\n",
    "- Logistic Regression remains the more **robust and generalizable model** for this dataset, especially in test-time performance.\n",
    "- Random Forest could benefit from **hyperparameter tuning or additional regularization**, as its cross-validation strength did not translate into superior real-world prediction.\n",
    "- These results reinforce the importance of balancing **interpretability**, **generalization**, and **validation rigor** in model evaluation — particularly in clinical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3085275c",
   "metadata": {},
   "source": [
    "## Visualizing Model Results\n",
    "\n",
    "To support interpretability and decision-making, we visualize key evaluation artifacts:\n",
    "- **Confusion Matrices** to inspect the types of classification errors\n",
    "- **ROC Curves** to assess threshold-independent discriminative performance\n",
    "- **Comparison Table** summarizing key metrics across both models\n",
    "\n",
    "These visuals help communicate not only how well each model performs, but also how they fail — guiding real-world deployment choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a855f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, roc_auc_score\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Ensure plots directory exists\n",
    "os.makedirs(\"../plots\", exist_ok=True)\n",
    "\n",
    "# --- Confusion Matrices ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, logreg_preds, ax=axes[0], cmap=\"Blues\", colorbar=False\n",
    ")\n",
    "axes[0].set_title(\"Logistic Regression\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, rf_preds, ax=axes[1], cmap=\"Greens\", colorbar=False\n",
    ")\n",
    "axes[1].set_title(\"Random Forest\")\n",
    "plt.suptitle(\"Confusion Matrices\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../plots/confusion_matrices.png\")\n",
    "plt.show()\n",
    "\n",
    "# Recalculate predicted probabilities for the positive class (Gallstones)\n",
    "# Confirm which class index corresponds to \"Gallstones\"\n",
    "gallstone_index = list(logreg.classes_).index(\"Gallstones\")\n",
    "logreg_probs = logreg.predict_proba(X_test_scaled)[:, gallstone_index]\n",
    "\n",
    "gallstone_index_rf = list(rf.classes_).index(\"Gallstones\")\n",
    "rf_probs = rf.predict_proba(X_test)[:, gallstone_index_rf]\n",
    "\n",
    "# Compute FPR, TPR, and AUC\n",
    "fpr_logreg, tpr_logreg, _ = roc_curve(y_test, logreg_probs, pos_label=\"Gallstones\")\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs, pos_label=\"Gallstones\")\n",
    "auc_logreg = roc_auc_score(y_test == \"Gallstones\", logreg_probs)\n",
    "auc_rf = roc_auc_score(y_test == \"Gallstones\", rf_probs)\n",
    "\n",
    "# Plot both curves\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr_logreg, tpr_logreg, color=\"tab:blue\")\n",
    "plt.plot(fpr_rf, tpr_rf, color=\"tab:orange\")\n",
    "\n",
    "# Inline colored labels\n",
    "plt.text(0, 0.8, f\"Logistic Regression\\n(AUC = {auc_logreg:.2f})\", color=\"tab:blue\")\n",
    "plt.text(0.4, 0.8, f\"Random Forest\\n(AUC = {auc_rf:.2f})\", color=\"tab:orange\")\n",
    "\n",
    "# Aesthetic settings\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves – Gallstones Detection\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../plots/roc_curves_inline_labels.png\")\n",
    "plt.show()\n",
    "\n",
    "# --- Metric Summary Table ---\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Binary version of y_test for AUC\n",
    "y_true_binary = (y_test == \"Gallstones\").astype(int)\n",
    "\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Logistic Regression\",\n",
    "        \"Accuracy\": accuracy_score(y_test, logreg_preds),\n",
    "        \"Precision\": precision_score(y_test, logreg_preds, pos_label=\"Gallstones\"),\n",
    "        \"Recall\": recall_score(y_test, logreg_preds, pos_label=\"Gallstones\"),\n",
    "        \"F1 Score\": f1_score(y_test, logreg_preds, pos_label=\"Gallstones\"),\n",
    "        \"ROC AUC\": roc_auc_score(y_true_binary, logreg_probs)\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Random Forest\",\n",
    "        \"Accuracy\": accuracy_score(y_test, rf_preds),\n",
    "        \"Precision\": precision_score(y_test, rf_preds, pos_label=\"Gallstones\"),\n",
    "        \"Recall\": recall_score(y_test, rf_preds, pos_label=\"Gallstones\"),\n",
    "        \"F1 Score\": f1_score(y_test, rf_preds, pos_label=\"Gallstones\"),\n",
    "        \"ROC AUC\": roc_auc_score(y_true_binary, rf_probs)\n",
    "    }\n",
    "])\n",
    "summary_df.set_index(\"Model\", inplace=True)\n",
    "summary_df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5017c3",
   "metadata": {},
   "source": [
    "## Model Evaluation Summary\n",
    "\n",
    "We conducted a side-by-side evaluation of Logistic Regression and Random Forest using multiple performance visualizations and classification metrics on the gallstone classification task.\n",
    "\n",
    "### ROC Curves\n",
    "- **Logistic Regression** produced the highest ROC AUC at **0.811**, indicating superior overall probability ranking of positive cases (Gallstones).\n",
    "- **Random Forest** achieved a slightly lower ROC AUC of **0.791**, suggesting it was less calibrated in distinguishing true positive rates at various thresholds.\n",
    "- Despite the intuitive appeal of Random Forest’s flexibility, the ROC curve confirms Logistic Regression generalized better on the test set.\n",
    "\n",
    "### Confusion Matrices\n",
    "- Logistic Regression correctly classified more \"No Gallstones\" cases with fewer false positives (9 vs. 11).\n",
    "- Random Forest achieved slightly better specificity, but at the cost of higher false negatives (missed Gallstone cases).\n",
    "- Both models showed balanced error rates, but Logistic Regression leaned slightly more conservative in its decision boundary.\n",
    "\n",
    "### Score Comparison Table\n",
    "| Metric     | Logistic Regression | Random Forest |\n",
    "|------------|---------------------|----------------|\n",
    "| Accuracy   | 0.750               | 0.703          |\n",
    "| Precision  | 0.767               | 0.724          |\n",
    "| Recall     | 0.719               | 0.656          |\n",
    "| F1 Score   | 0.742               | 0.689          |\n",
    "| ROC AUC    | 0.811               | 0.791          |\n",
    "\n",
    "### Interpretation\n",
    "- **Logistic Regression outperformed Random Forest** across nearly all evaluation metrics on the test set, with especially notable gains in recall and AUC.\n",
    "- For applications where interpretability and well-calibrated probabilities matter—such as risk communication with clinicians—Logistic Regression is an appropriate first-line model.\n",
    "- While Random Forest remains valuable due to its non-linearity and automatic feature selection, its lower test recall may limit usefulness in clinical screening contexts where false negatives carry high costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ce959",
   "metadata": {},
   "source": [
    "## Error Trade-Offs in Healthcare Context\n",
    "\n",
    "In the context of predicting gallstone presence, **false negatives** (predicting \"No Gallstones\" when gallstones are actually present) are typically more costly than false positives. A missed diagnosis could delay treatment, leading to worsening symptoms, emergency interventions, or complications such as pancreatitis.\n",
    "\n",
    "- **False Positives** may lead to further imaging or diagnostic procedures, which carry minor cost and inconvenience but low clinical risk.\n",
    "- **False Negatives**, however, risk allowing undetected gallstones to progress into acute conditions.\n",
    "\n",
    "### Implication:\n",
    "Model selection should prioritize **higher recall** to ensure most true gallstone cases are captured, even at the expense of a slight drop in precision. This favors Logistic Regression in our evaluation, which achieved the highest recall (0.719) while maintaining strong precision (0.767)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67f858",
   "metadata": {},
   "source": [
    "## Interpreting Model Behavior: Clinical and User Implications\n",
    "\n",
    "The models identified key features such as `crp_mg_l`, `vitamin_d_ng_ml`, and `alt_u_l` as important for predicting gallstone presence—variables that align with inflammation, nutritional status, and liver function, respectively.\n",
    "\n",
    "### Clinical Insights:\n",
    "- **CRP (C-Reactive Protein)** was the strongest predictor, supporting its role as a marker of inflammation relevant to biliary disease.\n",
    "- **Low Vitamin D** levels were associated with increased gallstone risk, reflecting emerging evidence linking deficiency with gallbladder dysfunction.\n",
    "- **ALT/AST Liver Enzymes** contribute meaningfully, consistent with liver-gallbladder axis involvement.\n",
    "\n",
    "### User and Stakeholder Relevance:\n",
    "- **Physicians** benefit from interpretable features (Logistic Regression) to support clinical reasoning and trust.\n",
    "- **Patients** gain from early risk identification and potential for preventive lifestyle or nutritional interventions.\n",
    "- **Decision Support Systems** should balance transparency (coefficients, feature importances) with performance to encourage adoption in clinical workflows.\n",
    "\n",
    "This interpretability reinforces the value of hybrid evaluation: blending performance with domain-informed insight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f6916",
   "metadata": {},
   "source": [
    "## Storytelling with Data – Chapter 3 Takeaways: \"Clutter is Your Enemy!\"\n",
    "\n",
    "1. **Reduce Non-Essential Visual Elements**  \n",
    "   Charts should guide attention, not overwhelm. Remove gridlines, borders, and redundant labels that don’t add value.\n",
    "\n",
    "2. **Use Color and Contrast Purposefully**  \n",
    "   Apply color to highlight key data—not decoratively. Reserve strong contrasts for emphasis, such as a critical metric or threshold.\n",
    "\n",
    "3. **Maximize Data-to-Ink Ratio**  \n",
    "   Every mark on a chart should serve a purpose. Simplify visuals to amplify the message and reduce distraction.\n",
    "\n",
    "These principles improve visual clarity and help ensure your audience absorbs the insights you intend to convey."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcmle-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
