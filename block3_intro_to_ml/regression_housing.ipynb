{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849fad58",
   "metadata": {},
   "source": [
    "# üè° Housing Prices ‚Äì Regression Model Training\n",
    "\n",
    "This notebook explores a supervised learning regression task using housing data.  \n",
    "We compare Decision Tree and Linear Regression models to predict house prices based on numeric features.  \n",
    "Later, we refine the models with better feature selection and introduce Random Forests to improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2827b",
   "metadata": {},
   "source": [
    "## 1. Loading Data\n",
    "\n",
    "We begin by importing libraries, loading the dataset, and performing an initial inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4542bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load the housing dataset\n",
    "file_path = 'data/housing.csv'  # Adjust if needed\n",
    "home_data = pd.read_csv(file_path)\n",
    "\n",
    "# Show dataset shape and preview top rows\n",
    "print(\"Shape:\", home_data.shape)\n",
    "home_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9530f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "home_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf59d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available columns\n",
    "print(\"Columns:\", home_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8c2d5",
   "metadata": {},
   "source": [
    "## 2. Feature Selection\n",
    "\n",
    "We identify relevant numeric features to use as inputs and define the target variable (`Price`).\n",
    "Categorical features will be explored in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7272a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "y = home_data['Price']\n",
    "\n",
    "# Select numeric features likely to impact house price\n",
    "feature_names = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Car']\n",
    "X = home_data[feature_names]\n",
    "\n",
    "# Preview selected features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572ab2c",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Selected features are all numeric and directly related to property size or structure.\n",
    "- These features are simple to work with initially and avoid preprocessing complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e61ccb4",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split\n",
    "\n",
    "We split the dataset into training and validation subsets using an 80/20 ratio.\n",
    "This allows us to evaluate model performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1081614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Confirm the split sizes\n",
    "print(\"Training set size:\", train_X.shape)\n",
    "print(\"Validation set size:\", val_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23742188",
   "metadata": {},
   "source": [
    "## 4. Decision Tree Regressor\n",
    "\n",
    "We train a baseline `DecisionTreeRegressor` using the selected numeric features.\n",
    "Performance is evaluated using Mean Absolute Error (MAE) on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa844de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Create and train the model\n",
    "tree_model = DecisionTreeRegressor(random_state=1)\n",
    "tree_model.fit(train_X, train_y)\n",
    "\n",
    "# Predict on validation data\n",
    "tree_preds = tree_model.predict(val_X)\n",
    "\n",
    "# Evaluate model performance\n",
    "tree_mae = mean_absolute_error(val_y, tree_preds)\n",
    "print(f\"Decision Tree MAE: {tree_mae:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141a8ac",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- MAE measures average absolute prediction error.\n",
    "- The Decision Tree fits the training data exactly but may not generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e2fab",
   "metadata": {},
   "source": [
    "## 5. Linear Regression\n",
    "\n",
    "We train a `LinearRegression` model after handling any missing values.  \n",
    "This model assumes linear relationships between input features and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7180f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Drop rows with missing values from training and validation sets\n",
    "train_X_clean = train_X.dropna()\n",
    "train_y_clean = train_y.loc[train_X_clean.index]\n",
    "\n",
    "val_X_clean = val_X.dropna()\n",
    "val_y_clean = val_y.loc[val_X_clean.index]\n",
    "\n",
    "# Train Linear Regression model\n",
    "linreg_model = LinearRegression()\n",
    "linreg_model.fit(train_X_clean, train_y_clean)\n",
    "\n",
    "# Predict and evaluate\n",
    "linreg_preds = linreg_model.predict(val_X_clean)\n",
    "linreg_mae = mean_absolute_error(val_y_clean, linreg_preds)\n",
    "\n",
    "print(f\"Linear Regression MAE: {linreg_mae:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735cb53",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Dropped rows with missing values to ensure training compatibility.\n",
    "- Linear Regression produced lower MAE than the Decision Tree, suggesting less overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b44db0",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n",
    "\n",
    "We compare validation MAE for both models.  \n",
    "Lower MAE indicates better average predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd838d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bar chart comparing model performance\n",
    "mae_values = [tree_mae, linreg_mae]\n",
    "models = ['Decision Tree', 'Linear Regression']\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(models, mae_values, color='skyblue')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Model Comparison: Validation MAE')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/housing_model_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3c8e9",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "| Model             | MAE       |\n",
    "|------------------|-----------|\n",
    "| Decision Tree     | 417,118   |\n",
    "| Linear Regression | 319,812   |\n",
    "\n",
    "- **Linear Regression** performed better, likely due to its simplicity and generalization ability.\n",
    "- **Decision Tree** likely overfitted the training data with full-depth splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db568c9a",
   "metadata": {},
   "source": [
    "## 7. Observations & Next Steps\n",
    "\n",
    "### Summary of Initial Models\n",
    "\n",
    "- **Decision Tree Regressor**\n",
    "  - Trained on all numeric features with default settings.\n",
    "  - Validation MAE: ~417K\n",
    "  - Likely overfitted ‚Äî sensitive to data noise and outliers.\n",
    "\n",
    "- **Linear Regression**\n",
    "  - Trained after dropping rows with missing values.\n",
    "  - Validation MAE: ~320K\n",
    "  - Outperformed the tree model, suggesting linear relationships exist in the data.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try refined feature sets to reduce noise.\n",
    "- Explore Random Forests to balance overfitting and underfitting.\n",
    "- Experiment with categorical features and data imputation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61902f",
   "metadata": {},
   "source": [
    "## 8. Model Refinement\n",
    "\n",
    "We now evaluate how different feature sets affect Decision Tree performance.\n",
    "This helps identify features that may add noise or improve predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd782fd0",
   "metadata": {},
   "source": [
    "### 8.1 Feature Set Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dbf882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance across multiple feature combinations\n",
    "feature_sets = {\n",
    "    \"Basic\": ['Rooms', 'Bathroom', 'Landsize'],\n",
    "    \"No Landsize\": ['Rooms', 'Bathroom', 'BuildingArea'],\n",
    "    \"With Car + YearBuilt\": ['Rooms', 'Bathroom', 'Car', 'YearBuilt'],\n",
    "    \"All Available\": ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Car']\n",
    "}\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae_results = {}\n",
    "\n",
    "for name, features in feature_sets.items():\n",
    "    X = home_data[features]\n",
    "    y = home_data['Price']\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    X = X.dropna()\n",
    "    y = y.loc[X.index]\n",
    "\n",
    "    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "    \n",
    "    model = DecisionTreeRegressor(random_state=1)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds)\n",
    "    \n",
    "    print(f\"{name} MAE: {mae:,.0f}\")\n",
    "    mae_results[name] = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature set performance\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(mae_results.keys(), mae_results.values(), color='skyblue')\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.title(\"Feature Set Comparison ‚Äì Housing Regression\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/housing_feature_set_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b754a",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "| Feature Set              | Validation MAE |\n",
    "|--------------------------|----------------|\n",
    "| Basic                    | 444,666        |\n",
    "| No Landsize              | 398,598        |\n",
    "| With Car + YearBuilt     | 339,981        |\n",
    "| All Available            | 372,995        |\n",
    "\n",
    "- **\"With Car + YearBuilt\"** produced the lowest MAE ‚Äî a strong candidate for further modeling.\n",
    "- **\"No Landsize\"** outperformed **\"Basic\"**, suggesting `Landsize` may introduce noise.\n",
    "- More features ‚â† better performance ‚Äî optimal subsets can outperform larger sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b3b0b",
   "metadata": {},
   "source": [
    "### 8.2 Random Forest Regression\n",
    "\n",
    "We now apply a `RandomForestRegressor`, an ensemble method that averages multiple decision trees.  \n",
    "This typically improves generalization and reduces overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b62f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Use the best-performing feature set: 'With Car + YearBuilt'\n",
    "X = home_data[['Rooms', 'Bathroom', 'Car', 'YearBuilt']]\n",
    "y = home_data['Price']\n",
    "\n",
    "# Drop missing values\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=1)\n",
    "rf_model.fit(train_X, train_y)\n",
    "\n",
    "# Predict and evaluate\n",
    "rf_preds = rf_model.predict(val_X)\n",
    "rf_mae = mean_absolute_error(val_y, rf_preds)\n",
    "\n",
    "print(f\"Random Forest MAE: {rf_mae:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "pd.Series(importances, index=features).sort_values().plot(\n",
    "    kind='barh', title=\"Random Forest Feature Importances\"\n",
    ")\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/housing_rf_feature_importance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2026fc",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- **Random Forest MAE:** ~277,538 ‚Äî the lowest so far.\n",
    "- Feature importances show `YearBuilt` and `Car` are strong contributors.\n",
    "- Ensemble methods significantly improved prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d3e46",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook developed and refined machine learning models to predict house prices using structured tabular data.\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "| Model               | MAE       |\n",
    "|--------------------|-----------|\n",
    "| Decision Tree       | 417,118   |\n",
    "| Linear Regression   | 319,812   |\n",
    "| Random Forest       | 277,538   |\n",
    "\n",
    "- The **Decision Tree Regressor** overfit the training data, resulting in higher MAE.\n",
    "- **Linear Regression** generalized better and improved performance by ~100K.\n",
    "- **Random Forest**, with a well-chosen subset of features (`Rooms`, `Bathroom`, `Car`, `YearBuilt`), delivered the best performance overall.\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "- **Feature selection** plays a critical role: eliminating noisy features like `Landsize` improved model accuracy.\n",
    "- **Simpler models** like Linear Regression can outperform more complex ones when data is clean and relationships are linear.\n",
    "- **Random Forests** improve generalization and reduce variance through ensemble learning.\n",
    "- **Visualization** (MAE comparison, feature importance) is essential for interpreting model behavior and guiding refinement.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Perform **hyperparameter tuning** (e.g., `n_estimators`, `max_depth`) for Random Forests.\n",
    "- Explore **categorical encoding** (e.g., Suburb, Type).\n",
    "- Try **cross-validation** to ensure robust performance estimates.\n",
    "- Implement **data imputation** to avoid dropping valuable rows with missing values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcmle-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
