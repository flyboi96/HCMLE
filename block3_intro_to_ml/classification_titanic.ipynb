{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0908e18",
   "metadata": {},
   "source": [
    "# Titanic Classification\n",
    "\n",
    "## 1. Loading Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset\n",
    "file_path = 'data/titanic.csv'  # adjust if needed\n",
    "titanic_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4da7e4",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View basic dataset shape\n",
    "print(\"Shape:\", titanic_data.shape)\n",
    "\n",
    "# Preview column names\n",
    "print(\"\\nColumns:\")\n",
    "print(titanic_data.columns.tolist())\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nTop rows:\")\n",
    "print(titanic_data.head())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(titanic_data.describe(include='all'))\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(titanic_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819ef9c",
   "metadata": {},
   "source": [
    "Initial inspection shows several columns with missing values (e.g., Age, Cabin). We'll decide how to handle them based on model needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa7d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns unlikely to help with classification or too incomplete\n",
    "titanic_data = titanic_data.drop(columns=['Cabin', 'Ticket', 'Name'])\n",
    "\n",
    "# Drop rows with missing target ('Survived') or critical features\n",
    "titanic_data = titanic_data.dropna(subset=['Survived', 'Embarked'])\n",
    "\n",
    "# Fill missing Age with median\n",
    "titanic_data['Age'] = titanic_data['Age'].fillna(titanic_data['Age'].median())\n",
    "\n",
    "# Encode 'Sex' and 'Embarked'\n",
    "titanic_data['Sex'] = titanic_data['Sex'].map({'male': 0, 'female': 1})\n",
    "titanic_data['Embarked'] = titanic_data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f22046",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "y = titanic_data['Survived']\n",
    "\n",
    "feature_names = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "X = titanic_data[feature_names]\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "titanic_data.to_csv('data/titanic_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc2a4f5",
   "metadata": {},
   "source": [
    "## Notes – Day 4\n",
    "\n",
    "- Loaded Titanic dataset and explored structure and missing values.\n",
    "- Dropped irrelevant columns (Name, Ticket, Cabin).\n",
    "- Filled missing Age values with median, dropped missing Embarked rows.\n",
    "- Encoded 'Sex' and 'Embarked' for modeling.\n",
    "- Selected 7 features to use in classification modeling tomorrow.\n",
    "- Dataset is clean and ready for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60349d",
   "metadata": {},
   "source": [
    "## Day 5 – Train/Test Split and Decision Tree Classification\n",
    "\n",
    "Today’s goal is to train a DecisionTreeClassifier to predict Titanic survival.  \n",
    "We'll evaluate its accuracy and interpret the model using a confusion matrix, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e030823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data = pd.read_csv('data/titanic_cleaned.csv')\n",
    "\n",
    "y = titanic_data['Survived']\n",
    "feature_names = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "X = titanic_data[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0041976",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = model.predict(val_X)\n",
    "accuracy = accuracy_score(val_y, val_predictions)\n",
    "print(f\"Validation Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd66d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(val_y, val_predictions)\n",
    "precision = precision_score(val_y, val_predictions)\n",
    "recall = recall_score(val_y, val_predictions)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8375e",
   "metadata": {},
   "source": [
    "## Day 5 Results – Titanic Classification\n",
    "\n",
    "- **Model:** DecisionTreeClassifier\n",
    "- **Validation Accuracy:** 80.3%\n",
    "- **Confusion Matrix:**\n",
    "\n",
    "| Actual \\ Predicted | 0 (Did Not Survive) | 1 (Survived) |\n",
    "|--------------------|---------------------|--------------|\n",
    "| **0 (Did Not Survive)** | 113                 | 25           |\n",
    "| **1 (Survived)**         | 19                  | 66           |\n",
    "\n",
    "- **Precision:** 72.5% – Of the predicted survivors, 72.5% actually survived.\n",
    "- **Recall:** 77.6% – The model correctly identified 77.6% of the actual survivors.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- The model performs reasonably well out of the box.\n",
    "- Slight bias toward predicting non-survivors (class 0), but catches most real survivors.\n",
    "- Future improvements could include:\n",
    "  - Trying a `max_depth` limit to reduce overfitting.\n",
    "  - Testing a RandomForestClassifier for better generalization.\n",
    "  - Exploring class imbalance solutions (e.g. balanced weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca66efe",
   "metadata": {},
   "source": [
    "## 6. Model Refinement and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fe9486",
   "metadata": {},
   "source": [
    "### Baseline Recap\n",
    "\n",
    "- Decision Tree Accuracy: 80.3%  \n",
    "- Precision: 72.5%  \n",
    "- Recall: 77.6%\n",
    "- Model used full depth — may overfit to training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61809b7e",
   "metadata": {},
   "source": [
    "## 6.1 Tree Depth Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "depths = [2, 3, 4, 5, 6, 7, 10, None]\n",
    "results = []\n",
    "\n",
    "for d in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=d, random_state=1)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds = model.predict(val_X)\n",
    "\n",
    "    acc = accuracy_score(val_y, preds)\n",
    "    prec = precision_score(val_y, preds)\n",
    "    rec = recall_score(val_y, preds)\n",
    "\n",
    "    results.append({\n",
    "        'Depth': d if d is not None else 'Full',\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec\n",
    "    })\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "depth_labels = [str(r['Depth']) for r in results]\n",
    "accs = [r['Accuracy'] for r in results]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(depth_labels, accs, marker='o', linestyle='-')\n",
    "plt.title(\"Validation Accuracy vs Tree Depth\")\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badafa5e",
   "metadata": {},
   "source": [
    "### 6.1 Tree Depth Tuning – Classification Performance Analysis\n",
    "\n",
    "We trained multiple `DecisionTreeClassifier` models on Titanic survival data with varying `max_depth` values. The goal was to observe how model complexity influences generalization performance.\n",
    "\n",
    "#### Validation Accuracy by Tree Depth\n",
    "\n",
    "| Max Depth | Accuracy |\n",
    "|-----------|----------|\n",
    "| 2         | 0.780    |\n",
    "| 3         | 0.843    |\n",
    "| 4         | 0.847    |\n",
    "| 5         | 0.857    |\n",
    "| 6         | 0.857    |\n",
    "| 7         | 0.853    |\n",
    "| 10        | 0.832    |\n",
    "| Full      | 0.803    |\n",
    "\n",
    "#### Observations\n",
    "\n",
    "- **Accuracy improved rapidly** from depth 2 to 5, with the peak at **depths 5 and 6 (≈85.7%)**.\n",
    "- Beyond depth 6, accuracy began to **decline**, indicating that **overfitting** began to outweigh the benefits of added complexity.\n",
    "- The fully grown tree underperformed compared to smaller trees, validating that unrestricted trees tend to memorize the training set rather than generalize.\n",
    "- Depths 5 and 6 offered the **best tradeoff between bias and variance**, capturing the structure in the data without overfitting noise.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Constraining tree depth is a simple yet effective form of **regularization** in decision trees. This experiment reinforces the importance of tuning model complexity to achieve optimal generalization. For this dataset, a depth of **5 or 6** is ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fbcce8",
   "metadata": {},
   "source": [
    "## 6.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337911f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Decision Tree with best depth\n",
    "dt_model = DecisionTreeClassifier(max_depth=5, random_state=1)\n",
    "dt_model.fit(train_X, train_y)\n",
    "dt_preds = dt_model.predict(val_X)\n",
    "\n",
    "# Store metrics\n",
    "dt_acc = accuracy_score(val_y, dt_preds)\n",
    "dt_prec = precision_score(val_y, dt_preds)\n",
    "dt_rec = recall_score(val_y, dt_preds)\n",
    "\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=1)\n",
    "rf_model.fit(train_X, train_y)\n",
    "\n",
    "# Predict and evaluate\n",
    "rf_preds = rf_model.predict(val_X)\n",
    "rf_acc = accuracy_score(val_y, rf_preds)\n",
    "rf_prec = precision_score(val_y, rf_preds)\n",
    "rf_rec = recall_score(val_y, rf_preds)\n",
    "rf_cm = confusion_matrix(val_y, rf_preds)\n",
    "\n",
    "print(f\"Accuracy: {rf_acc:.3f}\")\n",
    "print(f\"Precision: {rf_prec:.3f}\")\n",
    "print(f\"Recall: {rf_rec:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", rf_cm)\n",
    "\n",
    "print(f\"Decision Tree (d=5) → Accuracy: {dt_acc:.3f}, Precision: {dt_prec:.3f}, Recall: {dt_rec:.3f}\")\n",
    "print(f\"Random Forest       → Accuracy: {rf_acc:.3f}, Precision: {rf_prec:.3f}, Recall: {rf_rec:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cb995",
   "metadata": {},
   "source": [
    "### 6.2 Random Forest Classifier – Evaluation and Comparison\n",
    "\n",
    "We trained a `RandomForestClassifier` on the Titanic dataset and compared its performance to a tuned Decision Tree (`max_depth=5`).\n",
    "\n",
    "#### Results\n",
    "\n",
    "| Model                | Accuracy | Precision | Recall |\n",
    "|----------------------|----------|-----------|--------|\n",
    "| Decision Tree (d=5)  | 0.857    | 0.884     | 0.718  |\n",
    "| Random Forest        | 0.825    | 0.780     | 0.753  |\n",
    "\n",
    "**Random Forest Confusion Matrix**:\n",
    "```\n",
    "             Predicted\n",
    "             0      1\n",
    "Actual  0   120     18  \n",
    "        1    21     64  \n",
    "```\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- The **Decision Tree** achieved higher **overall accuracy** and **precision**, but at the cost of recall — it was more conservative in predicting survival, which reduced false positives but missed more true survivors.\n",
    "- The **Random Forest**, although slightly lower in accuracy and precision, achieved better **recall**, identifying a greater proportion of actual survivors. This behavior is expected from ensemble models, which tend to generalize better and reduce variance across predictions.\n",
    "- The confusion matrix confirms the tradeoff: Random Forest caught more positives (higher TP) at the cost of more false positives (lower precision).\n",
    "- These results illustrate a **typical precision–recall tradeoff**. The preferred model depends on context:\n",
    "  - Prioritize **precision** when false positives are costly (e.g. fraud detection).\n",
    "  - Prioritize **recall** when false negatives are riskier (e.g. medical diagnoses).\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Although the Decision Tree had stronger raw performance in accuracy and precision, the Random Forest produced more balanced recall. For generalization, the Random Forest remains competitive and should be further explored with hyperparameter tuning.\n",
    "\n",
    "Next steps:\n",
    "- Tune `n_estimators`, `max_depth`, and `max_features`\n",
    "- Evaluate using cross-validation\n",
    "- Consider additional classifiers (e.g., Logistic Regression, Gradient Boosting)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcmle-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
