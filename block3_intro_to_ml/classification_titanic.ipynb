{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0908e18",
   "metadata": {},
   "source": [
    "# 🛳️ Titanic Survival Classification – Supervised Learning Practice\n",
    "\n",
    "This notebook explores a classification task using the Titanic dataset.  \n",
    "We apply decision trees and random forests to predict passenger survival, with emphasis on model evaluation, tuning, and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766320f",
   "metadata": {},
   "source": [
    "## 1. Loading Data\n",
    "\n",
    "We begin by loading the raw Titanic dataset and performing basic checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file path and load Titanic dataset\n",
    "file_path = 'data/titanic.csv'  # Adjust path if needed\n",
    "titanic_data = pd.read_csv(file_path)\n",
    "\n",
    "# Preview dataset shape and column names\n",
    "print(\"Shape:\", titanic_data.shape)\n",
    "print(\"Columns:\", titanic_data.columns.tolist())\n",
    "\n",
    "# Preview the first few rows\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4da7e4",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "We explore the structure and quality of the dataset, identify missing values, and prepare for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics, including non-numeric columns\n",
    "titanic_data.describe(include='all')\n",
    "\n",
    "# Visualize missing values per column\n",
    "missing = titanic_data.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63aae9e",
   "metadata": {},
   "source": [
    "Initial inspection reveals:\n",
    "\n",
    "- `Cabin` is missing for most rows — we will drop it.\n",
    "- `Age` has moderate missingness — we will fill with the median.\n",
    "- `Embarked` has two missing values — we will drop those rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819ef9c",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "We'll remove irrelevant or highly incomplete columns, handle missing data, and encode categorical variables for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa7d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns unlikely to help with classification or too incomplete\n",
    "titanic_data = titanic_data.drop(columns=['Cabin', 'Ticket', 'Name'])\n",
    "\n",
    "# Drop rows with missing target ('Survived') or critical features\n",
    "titanic_data = titanic_data.dropna(subset=['Survived', 'Embarked'])\n",
    "\n",
    "# Fill missing Age with median\n",
    "titanic_data['Age'] = titanic_data['Age'].fillna(titanic_data['Age'].median())\n",
    "\n",
    "# Encode 'Sex' and 'Embarked'\n",
    "titanic_data['Sex'] = titanic_data['Sex'].map({'male': 0, 'female': 1})\n",
    "titanic_data['Embarked'] = titanic_data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "y = titanic_data['Survived']\n",
    "feature_names = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "X = titanic_data[feature_names]\n",
    "\n",
    "# Preview the final feature matrix and target\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f74cb3",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Removed columns `Cabin`, `Ticket`, and `Name` due to missingness or low predictive value.\n",
    "- Filled missing `Age` values with the median.\n",
    "- Encoded categorical features `Sex` and `Embarked`.\n",
    "- Selected 7 numeric features for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "titanic_data.to_csv('data/titanic_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60349d",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "We start by training a baseline `DecisionTreeClassifier` to predict passenger survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e030823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Split the dataset into training and validation sets (80/20 split)\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Train a Decision Tree classifier with default parameters\n",
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d656a5",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "### 5.1 Accuracy and Metrics\n",
    "We evaluate model performance using accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0041976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "# Predict on validation data\n",
    "val_predictions = model.predict(val_X)\n",
    "\n",
    "# Compute performance metrics\n",
    "accuracy = accuracy_score(val_y, val_predictions)\n",
    "precision = precision_score(val_y, val_predictions)\n",
    "recall = recall_score(val_y, val_predictions)\n",
    "cm = confusion_matrix(val_y, val_predictions)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8375e",
   "metadata": {},
   "source": [
    "### 5.2 Confusion Matrix\n",
    "\n",
    "| Actual \\\\ Predicted | 0 (Did Not Survive) | 1 (Survived) |\n",
    "|---------------------|---------------------|--------------|\n",
    "| 0                   | 113                 | 25           |\n",
    "| 1                   | 19                  | 66           |\n",
    "\n",
    "- **Precision:** 72.5% – Of those predicted to survive, 72.5% actually did.\n",
    "- **Recall:** 77.6% – The model correctly identified 77.6% of true survivors.\n",
    "\n",
    "The model performs well initially, though it shows mild overfitting. Further refinement can help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca66efe",
   "metadata": {},
   "source": [
    "## 6. Model Refinement and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fe9486",
   "metadata": {},
   "source": [
    "## 6. Model Refinement\n",
    "\n",
    "We now experiment with ways to improve the model:\n",
    "- Limit decision tree complexity using `max_depth`\n",
    "- Try a Random Forest ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61809b7e",
   "metadata": {},
   "source": [
    "### 6.1 Tree Depth Tuning\n",
    "\n",
    "To evaluate overfitting and underfitting, we trained several decision trees with increasing `max_depth` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "depths = [2, 3, 4, 5, 6, 7, 10, None]\n",
    "results = []\n",
    "\n",
    "# Evaluate each depth\n",
    "for d in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=d, random_state=1)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds = model.predict(val_X)\n",
    "\n",
    "    acc = accuracy_score(val_y, preds)\n",
    "    prec = precision_score(val_y, preds)\n",
    "    rec = recall_score(val_y, preds)\n",
    "\n",
    "    results.append({\n",
    "        'Depth': d if d is not None else 'Full',\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4ceb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation accuracy vs depth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "depth_labels = [str(r['Depth']) for r in results]\n",
    "accs = [r['Accuracy'] for r in results]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(depth_labels, accs, marker='o', linestyle='-')\n",
    "plt.title(\"Validation Accuracy vs Tree Depth\")\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/titanic_depth_accuracy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c4289",
   "metadata": {},
   "source": [
    "- Accuracy improved up to depth 5–6, peaking around 85.7%.\n",
    "- Beyond depth 6, the model began to overfit (lower validation accuracy).\n",
    "- A tree of depth 5 or 6 balances bias and variance effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fbcce8",
   "metadata": {},
   "source": [
    "### 6.2 Random Forest Classifier\n",
    "\n",
    "We compare the tuned decision tree to a Random Forest ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337911f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Train and evaluate Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=1)\n",
    "rf_model.fit(train_X, train_y)\n",
    "rf_preds = rf_model.predict(val_X)\n",
    "\n",
    "rf_acc = accuracy_score(val_y, rf_preds)\n",
    "rf_prec = precision_score(val_y, rf_preds)\n",
    "rf_rec = recall_score(val_y, rf_preds)\n",
    "rf_cm = confusion_matrix(val_y, rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best Decision Tree with max_depth=5\n",
    "dt_model = DecisionTreeClassifier(max_depth=5, random_state=1)\n",
    "dt_model.fit(train_X, train_y)\n",
    "dt_preds = dt_model.predict(val_X)\n",
    "\n",
    "dt_acc = accuracy_score(val_y, dt_preds)\n",
    "dt_prec = precision_score(val_y, dt_preds)\n",
    "dt_rec = recall_score(val_y, dt_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d446de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate and plot confusion matrix\n",
    "cm = confusion_matrix(val_y, rf_preds)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Did Not Survive', 'Survived'],\n",
    "            yticklabels=['Did Not Survive', 'Survived'])\n",
    "\n",
    "plt.title(\"Confusion Matrix – Random Forest\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/titanic_confusion_matrix.png\")  # Save to plots/\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9af573",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Decision Tree (d=5) → Accuracy: {dt_acc:.3f}, Precision: {dt_prec:.3f}, Recall: {dt_rec:.3f}\")\n",
    "print(f\"Random Forest       → Accuracy: {rf_acc:.3f}, Precision: {rf_prec:.3f}, Recall: {rf_rec:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2881c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Random Forest feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = train_X.columns\n",
    "\n",
    "pd.Series(importances, index=feature_names).sort_values().plot(\n",
    "    kind='barh', title=\"Random Forest Feature Importances\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/titanic_feature_importance.png\")  # Save to plots/\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cb995",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "| Model                | Accuracy | Precision | Recall |\n",
    "|----------------------|----------|-----------|--------|\n",
    "| Decision Tree (d=5)  | 0.857    | 0.884     | 0.718  |\n",
    "| Random Forest        | 0.825    | 0.780     | 0.753  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- The Decision Tree had stronger accuracy and precision but slightly lower recall.\n",
    "- The Random Forest improved recall, detecting more true positives at the cost of more false alarms.\n",
    "- This reflects a classic **precision–recall tradeoff**.\n",
    "\n",
    "We’ll finalize visuals and conclusions next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6155538e",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This notebook demonstrated a full supervised learning workflow on the Titanic dataset, applying both decision trees and random forests to predict passenger survival.\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "- **Baseline Decision Tree** (no tuning) achieved 80.3% accuracy.\n",
    "- **Tree depth tuning** showed that max depths of 5–6 provided the best tradeoff between underfitting and overfitting.\n",
    "- **Random Forest** slightly underperformed in overall accuracy but improved recall, showing better generalization.\n",
    "- **Feature engineering** (e.g., encoding Sex and Embarked, handling Age) significantly improved model readiness.\n",
    "- Visualizations (accuracy vs. depth, confusion matrix, feature importances) clarified model behavior.\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "- Precision and recall offer different insights; the ideal balance depends on application context.\n",
    "- Limiting model complexity is a simple but powerful way to reduce overfitting.\n",
    "- Ensembles like Random Forest are often more robust out-of-the-box, but tuning and interpretability remain important.\n",
    "\n",
    "---\n",
    "\n",
    "Next steps could include:\n",
    "- Hyperparameter tuning of `RandomForestClassifier` (e.g., `n_estimators`, `max_depth`)\n",
    "- Exploring additional features (e.g., family size, title from name)\n",
    "- Testing logistic regression or gradient boosting for comparison\n",
    "- Performing cross-validation for more reliable evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcmle-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
