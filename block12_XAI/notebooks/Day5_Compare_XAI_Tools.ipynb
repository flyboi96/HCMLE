{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b862b865",
   "metadata": {},
   "source": [
    "## Comparing XAI Tools: LIME vs SHAP vs PDP\n",
    "\n",
    "### Quick Overview (Layman’s Explanation)\n",
    "\n",
    "| Method | What It Does | How It Works | When to Use |\n",
    "|--------|---------------|--------------|--------------|\n",
    "| **LIME** *(Local Interpretable Model-agnostic Explanations)* | Explains a single prediction by training a simple, human-friendly model near that data point. | It perturbs the data slightly and fits a small linear model to approximate the complex model locally. | When you need a quick, interpretable snapshot of *why* a model made a particular decision. |\n",
    "| **SHAP** *(SHapley Additive exPlanations)* | Explains how much each feature contributed to a prediction, using principles from cooperative game theory. | It fairly distributes the “credit” (positive or negative) of a prediction among all input features. | When you want reliable, consistent, and mathematically grounded local or global explanations. |\n",
    "| **PDP** *(Partial Dependence Plot)* | Shows how changing one feature affects the model’s average prediction, holding others constant. | It varies one (or two) features across a range and plots the mean model output. | When you want to understand overall trends or feature effects across the entire dataset. |\n",
    "\n",
    "---\n",
    "\n",
    "### LIME vs SHAP (Local Explanations)\n",
    "\n",
    "- LIME builds a *local surrogate model* (like a small linear regression) to approximate the black box’s behavior near one instance, while SHAP directly attributes each feature’s contribution using game theory.  \n",
    "- SHAP explanations are generally **more consistent and reproducible** than LIME, which can vary due to random sampling or kernel width.  \n",
    "- LIME is **faster and easier to compute** but less theoretically rigorous; SHAP is **slower but more precise and principled**.  \n",
    "- LIME’s simple linear weights are **intuitive for non-technical audiences**, while SHAP offers **quantitatively accurate, model-agnostic attributions** for deeper analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### SHAP vs PDP (Global Interpretability)\n",
    "\n",
    "- SHAP can operate both **locally and globally**, while PDP focuses purely on **global trends** across all samples.  \n",
    "- PDP assumes **feature independence**, which can be misleading when features interact; SHAP naturally accounts for **feature interactions and dependencies**.  \n",
    "- PDP is **visually intuitive** and great for showing general relationships, but SHAP provides **richer, instance-level insights** that can be aggregated for global summaries.\n",
    "\n",
    "---\n",
    "\n",
    "### Tradeoffs and Limitations\n",
    "\n",
    "- **SHAP:** Most accurate and trustworthy, but computationally expensive—especially for complex models.  \n",
    "- **LIME:** Lightweight and flexible, but explanations can be unstable or sensitive to hyperparameters.  \n",
    "- **PDP:** Excellent for big-picture understanding, but not reliable for explaining individual predictions or correlated features.  \n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "Each tool serves a distinct role:\n",
    "- **Use LIME** when you need quick, easy-to-understand local explanations.  \n",
    "- **Use SHAP** when you want consistent, theoretically grounded insights across both local and global levels.  \n",
    "- **Use PDP** when your goal is to understand *overall feature effects* rather than individual predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee7f33",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e51088e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
