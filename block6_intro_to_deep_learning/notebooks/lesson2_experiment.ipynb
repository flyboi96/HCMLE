{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "# Dataset\n",
    "path = untar_data(URLs.PETS)\n",
    "path_img = path / \"images\"\n",
    "\n",
    "def is_cat(x): return Path(x).name[0].isupper()\n",
    "\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path_img,\n",
    "    get_image_files(path_img),\n",
    "    valid_pct=0.2,\n",
    "    seed=42,\n",
    "    label_func=is_cat,\n",
    "    item_tfms=Resize(224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d83eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=accuracy)\n",
    "\n",
    "# EXPERIMENT: Use 5 epochs and a custom learning rate\n",
    "learn.fine_tune(5, base_lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0981f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=accuracy)\n",
    "learn.fine_tune(3, base_lr=1e-1)  # Intentionally too high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a11f6",
   "metadata": {},
   "source": [
    "## Hyperparameter Experimentation: Epochs and Learning Rate\n",
    "\n",
    "### Objective\n",
    "The goal of this experiment was to analyze the effect of modifying training hyperparameters—specifically the number of epochs and the learning rate—on model performance and training stability.\n",
    "\n",
    "---\n",
    "\n",
    "### Baseline (3 epochs, default LR = 3e-3)\n",
    "\n",
    "| Metric        | Final Value |\n",
    "|---------------|-------------|\n",
    "| Accuracy      | 0.9959      |\n",
    "| Valid Loss    | 0.0093      |\n",
    "\n",
    "Using the fastai default learning rate (`3e-3`) with 3 epochs, the model achieved high accuracy (~99.6%) and very low validation loss. This establishes a strong baseline for comparison.\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment 1: Lower LR (1e-4), More Epochs (5)\n",
    "\n",
    "| Metric        | Final Value |\n",
    "|---------------|-------------|\n",
    "| Accuracy      | 0.9932      |\n",
    "| Valid Loss    | 0.0160      |\n",
    "\n",
    "Despite a longer training period (5 epochs), reducing the learning rate led to **slower convergence**. While the model maintained high accuracy, it underperformed slightly compared to the baseline in both accuracy and loss. This reflects the tradeoff of a conservative learning rate: **greater training stability at the cost of learning speed**. Interestingly, the model achieved ~99.3% accuracy by epoch 2, with marginal improvement thereafter, suggesting **early stopping** could have been beneficial.\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment 2: High LR (1e-1), 3 Epochs\n",
    "\n",
    "| Metric        | Final Value |\n",
    "|---------------|-------------|\n",
    "| Accuracy      | 0.9756      |\n",
    "| Valid Loss    | 0.0721      |\n",
    "\n",
    "Using an aggressively high learning rate (`1e-1`) destabilized training. The model exhibited:\n",
    "- Extremely high validation loss in epoch 0 (`3.68`)\n",
    "- Erratic loss curves\n",
    "- Delayed accuracy improvements\n",
    "\n",
    "Despite eventually recovering to ~97.6% accuracy, this configuration underperformed both in stability and overall validation performance. The result aligns with expectations: **a learning rate too high overshoots optimal weights**, leading to poor generalization and instability.\n",
    "\n",
    "---\n",
    "\n",
    "### Takeaways\n",
    "\n",
    "- The **default fastai setting (3e-3, 3 epochs)** yielded the best generalization and fastest convergence.\n",
    "- A **lower LR (1e-4)** can be beneficial for highly noisy data or fine-grained tuning, but may not outperform the baseline without increased epochs or regularization.\n",
    "- A **high LR (1e-1)** is unsafe without a robust LR scheduler or gradient clipping—it leads to high variance and risk of divergence.\n",
    "- **Hyperparameter tuning should balance convergence speed, stability, and validation performance**, and decisions should be guided by empirical trends in loss curves and accuracy saturation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hcmle-env)",
   "language": "python",
   "name": "hcmle-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
