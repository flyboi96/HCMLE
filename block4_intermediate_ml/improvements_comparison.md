# ðŸ“ˆ Model Improvement Summary â€” Block 4

This report documents the progressive improvement of a housing price prediction model through structured experimentation and pipeline optimization. The comparison spans from an initial baseline linear model to a tuned Random Forest evaluated on a hold-out test set.

---

## ðŸ” Before vs. After: Performance Metrics

| Model                               | MAE (AUD)     | RMSE (AUD)    | RÂ² Score |
|------------------------------------|---------------|---------------|----------|
| Baseline (Block 3: Linear Model)   | \$262,179     | N/A           | ~0.59    |
| Basic Decision Tree                | \$162,179     | N/A           | ~0.72    |
| Cross-Validated Pipeline (Day 5)   | \$136,392     | \$247,778     | 0.850    |
| Tuned Model (CV Mean, Day 6)       | \$136,362     | N/A           | N/A      |
| **Final Hold-Out Model (Day 7)**   | **\$135,083** | **\$234,516** | **0.862** |

> ðŸ“Œ Note: Final evaluation conducted on a hold-out test set (20%) to validate generalization.

---

## ðŸ“Š Improvements by Phase

### ðŸ“Œ Block 3 â†’ Block 4 Day 1â€“2
- Replaced naive row-dropping with a proper preprocessing pipeline.
- Imputed missing values using statistical strategies.
- Introduced encoding for categorical variables via `OneHotEncoder`.

### ðŸ“Œ Day 3â€“4
- Built scikit-learn pipelines with `ColumnTransformer` for modularity.
- Engineered domain-specific features:
  - `TotalRooms` (sum of bedrooms, bathrooms, and rooms)
  - `HouseAge` (2025 - YearBuilt)
  - `PricePerSqm` (for analysis)

### ðŸ“Œ Day 5â€“6
- Switched to **Random Forest Regressor** for non-linear modeling.
- Introduced robust evaluation: `MAE`, `RMSE`, `RÂ²` via 5-fold CV.
- Tuned hyperparameters (`max_depth`, `n_estimators`) via `GridSearchCV`.

### ðŸ“Œ Day 7
- Trained best model on full data and evaluated on unseen hold-out set.
- Achieved lowest MAE and RMSE while maintaining high RÂ².

---

## ðŸ§  Key Learnings

- **Feature engineering** had the single biggest impact on reducing error (~\$100K MAE drop).
- **Cross-validation** and **model tuning** further reduced error and stabilized predictions.
- **Ensemble methods** significantly outperformed linear baselines, especially in non-linear data regimes.

---

## ðŸ§° Final Model Artifacts

- âœ… `final_model.ipynb`: End-to-end notebook with training and evaluation
- âœ… `model_tuning.ipynb`: Grid search tuning and diagnostics
- âœ… `model_evaluation.ipynb`: Metric visualizations and interpretation
- âœ… `feature_engineering.ipynb`: Feature construction pipeline

---

## âœ… Conclusion

Block 4 successfully demonstrated how methodical ML developmentâ€”focusing on **pipelines**, **evaluation**, **feature engineering**, and **tuning**â€”can yield high-performance, production-ready models. The lessons and techniques in this block are foundational to human-centered machine learning engineering and real-world deployable systems.