{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7046f877",
   "metadata": {},
   "source": [
    "# üìà Model Evaluation: Metrics & Cross-Validation\n",
    "\n",
    "This notebook analyzes model performance using multiple evaluation metrics, including MAE, RMSE, and R¬≤.  \n",
    "We apply 5-fold cross-validation and plot parameter sensitivity to support future tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a76c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/housing.csv\")\n",
    "\n",
    "X = df.drop(\"Price\", axis=1)\n",
    "y = df[\"Price\"]\n",
    "\n",
    "# Feature engineering (reuse Day 4)\n",
    "X = X.copy()\n",
    "X[\"TotalRooms\"] = X[\"Bedroom2\"] + X[\"Bathroom\"] + X[\"Rooms\"]\n",
    "X[\"HouseAge\"] = 2025 - X[\"YearBuilt\"]\n",
    "X[\"PricePerSqm\"] = df[\"Price\"] / (X[\"BuildingArea\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "mae_scores = -cross_val_score(pipeline, X, y, cv=cv, scoring=\"neg_mean_absolute_error\")\n",
    "rmse_scores = (-cross_val_score(pipeline, X, y, cv=cv, scoring=\"neg_root_mean_squared_error\"))\n",
    "r2_scores = cross_val_score(pipeline, X, y, cv=cv, scoring=\"r2\")\n",
    "\n",
    "print(f\"Mean MAE: ${mae_scores.mean():,.0f}\")\n",
    "print(f\"Mean RMSE: ${rmse_scores.mean():,.0f}\")\n",
    "print(f\"Mean R¬≤: {r2_scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a1ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "axs[0].plot(mae_scores, marker='o', linestyle='--')\n",
    "axs[0].set_title(\"MAE per Fold\")\n",
    "axs[0].set_ylabel(\"MAE\")\n",
    "\n",
    "axs[1].plot(rmse_scores, marker='o', linestyle='--')\n",
    "axs[1].set_title(\"RMSE per Fold\")\n",
    "axs[1].set_ylabel(\"RMSE\")\n",
    "\n",
    "axs[2].plot(r2_scores, marker='o', linestyle='--')\n",
    "axs[2].set_title(\"R¬≤ per Fold\")\n",
    "axs[2].set_ylabel(\"R¬≤ Score\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"Fold\")\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c3369",
   "metadata": {},
   "source": [
    "## üìä Evaluation Summary: Model Performance Across Folds\n",
    "\n",
    "To assess the reliability and generalization capability of the current Random Forest model, we evaluated it using 5-fold cross-validation across three key regression metrics: **MAE**, **RMSE**, and **R¬≤**.\n",
    "\n",
    "### üî¢ Results Overview\n",
    "\n",
    "- **Mean MAE:** \\$136,392 AUD  \n",
    "- **Mean RMSE:** \\$247,778 AUD  \n",
    "- **Mean R¬≤ Score:** 0.850\n",
    "\n",
    "The relatively low MAE and RMSE indicate that the model makes consistent, moderate prediction errors across the test folds. The R¬≤ value of 0.850 suggests that the model explains ~85% of the variance in housing prices ‚Äî a strong fit for a real-world dataset with inherent noise and outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Metric-by-Metric Interpretation\n",
    "\n",
    "#### üìå **MAE (Mean Absolute Error)**\n",
    "- Shows low fold-to-fold variance (within ~\\$10K)\n",
    "- Indicates consistent performance without extreme prediction outliers\n",
    "- Interpretable as: on average, the model misses by ~\\$136K AUD per prediction\n",
    "\n",
    "#### üìå **RMSE (Root Mean Squared Error)**\n",
    "- Higher than MAE, as expected, due to penalizing large errors more strongly\n",
    "- Slight variance between folds suggests occasional larger residuals (likely due to luxury homes or data sparsity)\n",
    "- Useful for downstream decision-makers who want to penalize larger misses more harshly\n",
    "\n",
    "#### üìå **R¬≤ (Coefficient of Determination)**\n",
    "- Values across folds range from ~0.82 to ~0.86\n",
    "- Consistently high R¬≤ indicates stable predictive power regardless of how data is partitioned\n",
    "- Confirms that the model generalizes well across different subsets of the data\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Diagnostic Insights\n",
    "\n",
    "- **Fold 1 (RMSE ~280K)** shows a spike ‚Äî likely driven by a small number of expensive properties that dominate fold-specific error. This suggests that additional robustness (e.g., log-transformed targets or stratified folds) may help.\n",
    "- **Fold 3 yielded the lowest MAE and RMSE**, suggesting optimal balance of typical home distributions and minimal outlier influence.\n",
    "- **MAE stability across folds** (compared to RMSE) reaffirms that outlier error does not significantly skew the overall performance.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Next Steps\n",
    "\n",
    "- Investigate outlier impact on Fold 1 via residual plots or leverage statistics\n",
    "- Consider normalizing or log-transforming `Price` to stabilize RMSE\n",
    "- Use these metrics and plots as baseline for hyperparameter tuning (Day 6)\n",
    "\n",
    "This evaluation confirms that the model‚Äôs strong performance is not isolated to a single split ‚Äî a critical step in developing models suitable for real-world deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c277df",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [3, 5, 7, 10, None]\n",
    "mae_results = []\n",
    "\n",
    "for d in depths:\n",
    "    model = RandomForestRegressor(max_depth=d, n_estimators=100, random_state=42)\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    scores = -cross_val_score(pipeline, X, y, scoring='neg_mean_absolute_error', cv=cv)\n",
    "    mae_results.append(scores.mean())\n",
    "\n",
    "plt.plot(depths[:-1] + [\"None\"], mae_results, marker=\"o\")\n",
    "plt.title(\"MAE vs. max_depth\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806b9bc",
   "metadata": {},
   "source": [
    "## üìâ Parameter Sensitivity Analysis: `max_depth` vs MAE\n",
    "\n",
    "To explore how tree complexity affects model performance, we evaluated `max_depth` as a tuning parameter for `RandomForestRegressor`, using 5-fold cross-validation and MAE as the evaluation metric.\n",
    "\n",
    "### üîß Parameter Range Tested\n",
    "- `max_depth ‚àà {3, 5, 7, 10, None}`\n",
    "\n",
    "### üß™ Observed Performance (MAE)\n",
    "| max_depth | Mean MAE (AUD) |\n",
    "|-----------|----------------|\n",
    "| 3         | ~\\$292,000     |\n",
    "| 5         | ~\\$232,000     |\n",
    "| 7         | ~\\$195,000     |\n",
    "| 10        | ~\\$161,000     |\n",
    "| None      | ~\\$137,000     |\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Interpretation\n",
    "\n",
    "- **Model performance improved monotonically** as `max_depth` increased. Shallower trees (e.g., depth 3 or 5) significantly underfit the data, leading to high MAE values.\n",
    "- **Deeper trees (‚â•10)** captured more complex, nonlinear patterns in the data and achieved substantially lower error.\n",
    "- The best-performing model had **no depth restriction (`max_depth=None`)**, yielding an MAE near \\$137,000 AUD ‚Äî closely aligned with earlier full pipeline performance.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Implications\n",
    "\n",
    "- While deeper trees offer improved accuracy in this case, they may also increase risk of **overfitting** if noise or irrelevant features are present.\n",
    "- In Random Forests, the ensemble effect mitigates overfitting compared to single trees, but tuning still matters ‚Äî especially for performance scaling and training efficiency.\n",
    "- This trend justifies further exploration of **max_depth**, **min_samples_leaf**, and **n_estimators** using grid search or randomized search in Day 6.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Next Steps\n",
    "\n",
    "- Formalize this insight into a hyperparameter tuning routine (e.g., `GridSearchCV`) with multiple simultaneous parameters.\n",
    "- Consider testing whether `max_depth=None` generalizes across other random seeds or fold configurations.\n",
    "- Monitor **fit time and model size** if moving toward deployment or resource-constrained environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcmle-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
