{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a80134",
   "metadata": {},
   "source": [
    "# ðŸ”§ Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "This notebook performs hyperparameter tuning on the Random Forest Regressor using `GridSearchCV`.  \n",
    "The goal is to find the optimal combination of `max_depth` and `n_estimators` to minimize MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f755cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/housing.csv\")\n",
    "\n",
    "X = df.drop(\"Price\", axis=1).copy()\n",
    "y = df[\"Price\"]\n",
    "\n",
    "X[\"TotalRooms\"] = X[\"Bedroom2\"] + X[\"Bathroom\"] + X[\"Rooms\"]\n",
    "X[\"HouseAge\"] = 2025 - X[\"YearBuilt\"]\n",
    "X[\"PricePerSqm\"] = df[\"Price\"] / (X[\"BuildingArea\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d7df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, numeric_cols),\n",
    "    (\"cat\", categorical_transformer, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f69b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"model__n_estimators\": [50, 100, 200],\n",
    "    \"model__max_depth\": [5, 10, 15, None]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f24b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4802841",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "best_mae = -grid_search.best_score_\n",
    "print(f\"Best MAE: ${best_mae:,.0f} AUD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245da76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results = results.sort_values(\"rank_test_score\")\n",
    "results[[\"param_model__n_estimators\", \"param_model__max_depth\", \"mean_test_score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = results.pivot(index=\"param_model__max_depth\", columns=\"param_model__n_estimators\", values=\"mean_test_score\")\n",
    "pivot = -pivot  # convert to positive MAE\n",
    "\n",
    "pivot.plot(kind=\"line\", marker=\"o\", title=\"MAE vs. max_depth and n_estimators\", figsize=(10,6))\n",
    "plt.ylabel(\"Mean MAE (AUD)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e3aa8",
   "metadata": {},
   "source": [
    "## ðŸ”§ Model Tuning Summary: GridSearchCV on Random Forest\n",
    "\n",
    "This notebook conducted a hyperparameter search over 12 configurations of a `RandomForestRegressor` using 5-fold cross-validation and **Mean Absolute Error (MAE)** as the evaluation metric.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Grid Search Configuration\n",
    "\n",
    "- **Model:** RandomForestRegressor\n",
    "- **Evaluation Metric:** `neg_mean_absolute_error`\n",
    "- **Cross-Validation:** 5-fold, shuffled with `random_state=42`\n",
    "- **Hyperparameters Tuned:**\n",
    "  - `n_estimators âˆˆ {50, 100, 200}`\n",
    "  - `max_depth âˆˆ {5, 10, 15, None}`\n",
    "\n",
    "This results in 12 hyperparameter combinations tested for model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ¥‡ Best Model Performance\n",
    "\n",
    "| Hyperparameter        | Value      |\n",
    "|-----------------------|------------|\n",
    "| `n_estimators`        | 200        |\n",
    "| `max_depth`           | None       |\n",
    "| **Mean CV MAE**       | \\$136,362 AUD |\n",
    "\n",
    "The best-performing configuration used **200 estimators** and no constraint on tree depth. This model achieved the lowest MAE across folds, improving upon earlier results from Day 4 and Day 5.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ˆ Observations\n",
    "\n",
    "- Increasing both `n_estimators` and `max_depth` generally led to better performance, with diminishing returns beyond `max_depth=15`.\n",
    "- The most complex models (deep trees + many estimators) did not overfit under cross-validation but may be computationally heavier for deployment.\n",
    "- Shallower models (`max_depth=5`) exhibited underfitting, as reflected in MAE values > \\$175,000 AUD.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“‰ MAE Trend Visualization\n",
    "\n",
    "The MAE surface plotted against `max_depth` and `n_estimators` showed:\n",
    "\n",
    "- A **monotonic decrease** in error as `max_depth` increases.\n",
    "- **Marginal improvements** moving from 100 to 200 estimators, suggesting that ensemble size is helpful but saturates.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Interpretation\n",
    "\n",
    "The combination of deep trees and large ensemble size gave the most accurate predictions, reducing MAE by nearly **\\$26,000 AUD** compared to the untuned model from Day 5 (MAE â‰ˆ \\$162,391 AUD).\n",
    "\n",
    "This improvement confirms that careful hyperparameter optimization can meaningfully enhance model accuracy even in robust learners like Random Forests.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Next Steps\n",
    "\n",
    "- Use this tuned configuration for final model training and evaluation in Day 7.\n",
    "- Optionally explore tuning `min_samples_leaf`, `min_samples_split`, or `max_features`.\n",
    "- Consider using `RandomizedSearchCV` for larger hyperparameter spaces to improve compute efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcmle-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
