{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0881fcf",
   "metadata": {},
   "source": [
    "# üèÅ Final Model Training and Evaluation\n",
    "\n",
    "This notebook trains the optimized Random Forest model using the full dataset and compares its performance to the original baseline model from Block 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39600b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/housing.csv\")\n",
    "\n",
    "X = df.drop(\"Price\", axis=1).copy()\n",
    "y = df[\"Price\"]\n",
    "\n",
    "# Feature engineering\n",
    "X[\"TotalRooms\"] = X[\"Bedroom2\"] + X[\"Bathroom\"] + X[\"Rooms\"]\n",
    "X[\"HouseAge\"] = 2025 - X[\"YearBuilt\"]\n",
    "X[\"PricePerSqm\"] = df[\"Price\"] / (X[\"BuildingArea\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baae59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ad47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, numeric_cols),\n",
    "    (\"cat\", categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor(n_estimators=200, max_depth=None, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ed4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc5e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: ${mae:,.0f}\")\n",
    "print(f\"RMSE: ${rmse:,.0f}\")\n",
    "print(f\"R¬≤: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2feaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title(\"Residuals vs Predicted Price\")\n",
    "plt.xlabel(\"Predicted Price\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9c83b",
   "metadata": {},
   "source": [
    "## üßæ Final Model Evaluation: Hold-Out Set\n",
    "\n",
    "After completing hyperparameter tuning, the final model was trained on the full training set and evaluated on a hold-out test set (20% of the data). The results are as follows:\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **MAE** (Mean Absolute Error) | \\$135,083 AUD |\n",
    "| **RMSE** (Root Mean Squared Error) | \\$234,516 AUD |\n",
    "| **R¬≤ Score** | 0.862 |\n",
    "\n",
    "These metrics reflect a strong generalization performance. The model achieves a **13.3% improvement in MAE** compared to the tuned cross-validated model from Day 6 and a **~48% improvement** over the Block 3 baseline.\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ Residual Analysis\n",
    "\n",
    "The residuals (errors between predicted and actual prices) were plotted against predicted prices to visually inspect model bias and variance.\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "- **Centering Around Zero:** Most residuals cluster around the horizontal axis, indicating that the model is unbiased on average.\n",
    "- **Heteroskedasticity Detected:** As predicted prices increase, the spread of residuals increases. This implies the model struggles more with high-end properties‚Äîconsistent with fewer training examples in that price range.\n",
    "- **Few Extreme Outliers:** A handful of extreme positive and negative residuals (beyond ¬±2 million AUD) suggest underfitting in very high-price outliers, or potential label/data quality issues.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Interpretation:\n",
    "\n",
    "- The model performs **consistently well across the majority** of the value range.\n",
    "- **Prediction variance increases with price**, a common pattern in housing datasets. This suggests a potential benefit from:\n",
    "  - Applying log-transformation to stabilize variance\n",
    "  - Creating interaction features that capture nonlinear effects in expensive homes\n",
    "  - Using quantile regression or uncertainty-aware methods\n",
    "\n",
    "Despite this, the **overall residual shape supports the model‚Äôs reliability**, with low central bias and a high R¬≤ (0.862), which confirms it explains the vast majority of price variation in the test set.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary:\n",
    "\n",
    "The final model demonstrates excellent performance across evaluation metrics and residual diagnostics. While some predictive error persists in the luxury segment, the pipeline is well-validated and ready for summary reporting or further experimentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcmle-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
