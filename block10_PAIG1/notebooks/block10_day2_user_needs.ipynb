{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794c8cd3",
   "metadata": {},
   "source": [
    "# Day 2 – User Needs & Defining Success\n",
    "\n",
    "## 1. Objective\n",
    "Understand and articulate how user needs should define AI system success.\n",
    "\n",
    "## 2. Key Steps\n",
    "- Read Chapter 1 of People + AI Guidebook\n",
    "- Completed worksheet-style reflection\n",
    "- Drafted AI fit and user-centered reward function summary\n",
    "\n",
    "## 3. Results\n",
    "- Defined user problem and AI fit for chosen project\n",
    "- Outlined success criteria from a human perspective\n",
    "- Saved worksheet and reward function summary in notebook\n",
    "\n",
    "## 4. Summary\n",
    "- AI must solve a real user problem and deliver measurable human impact\n",
    "- Success = alignment between user outcome and system behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ecabab",
   "metadata": {},
   "source": [
    "## Chapter 1 – Key Themes from People + AI Guidebook\n",
    "\n",
    "### 1. Identify User Needs and AI Strengths\n",
    "\n",
    "A successful AI system begins by addressing a **real, unmet user need**. Before choosing a model or dataset, ask: *What problem are users actually trying to solve?*  \n",
    "Not all problems require AI — sometimes simpler solutions are better. Only use AI when its strengths (e.g., pattern recognition, scalability, personalization) align with the nature of the user problem. The AI must **amplify user ability**, not distract from it.\n",
    "\n",
    "> **Design prompt:** What is the user's goal, and is AI uniquely helpful here?\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Balance Automation and Augmentation\n",
    "\n",
    "Decide whether the AI should act **autonomously** or **support the user as an assistant**. Some systems work best when they fully automate a task (e.g., sorting spam), while others require human oversight, input, or judgment (e.g., medical diagnosis tools).  \n",
    "Designers should think in terms of **collaboration** — creating systems that leverage the best of both humans and machines. Often, partial automation with human-in-the-loop workflows leads to safer, more trusted outcomes.\n",
    "\n",
    "> **Design prompt:** What should the human still control or decide?\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Define Interaction Design Policies\n",
    "\n",
    "AI systems frequently encounter **ambiguity**, uncertainty, or edge cases. A well-designed product anticipates these moments and has clear interaction policies for them. For example:\n",
    "- What should the system do when it’s uncertain?\n",
    "- Should it guess, ask the user, defer, or provide options?\n",
    "\n",
    "Interaction design policies define how the system behaves in such situations, shaping **user trust and experience**. It's critical to make these policies **predictable, explainable, and user-aligned**.\n",
    "\n",
    "> **Design prompt:** When the AI is unsure, how does it behave?\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Prepare for Potential Pitfalls\n",
    "\n",
    "No system is perfect. AI systems can fail in ways that frustrate, confuse, or harm users — especially if failure isn't transparent. Designers must plan for:\n",
    "- **Unintended consequences**\n",
    "- **Bias or misuse**\n",
    "- **Misaligned incentives**\n",
    "- **Failure handling**\n",
    "\n",
    "Preparing for these pitfalls involves scenario testing, stress cases, and fallback strategies. It's not just about **minimizing error**, but about **maximizing the user's ability to recover and understand** what’s happening.\n",
    "\n",
    "> **Design prompt:** What does failure look like, and how does the system handle it?\n",
    "\n",
    "---\n",
    "\n",
    "### Unifying Principle:\n",
    "Success is not defined by accuracy alone. It’s defined by whether the system **helps real users achieve their goals, safely and effectively**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88cab4d",
   "metadata": {},
   "source": [
    "## Applied Example: CalcMentor\n",
    "\n",
    "- **Target User:**  \n",
    "College students struggling with calculus\n",
    "\n",
    "- **User Need:**  \n",
    "Students need timely, trustworthy math help outside of class\n",
    "\n",
    "- **Can this be solved without AI?**  \n",
    "Yes, via human tutors — but scalability and availability are limited\n",
    "\n",
    "- **AI Strengths Fit:**  \n",
    "Natural language understanding for Q&A, instant access, 24/7\n",
    "\n",
    "- **Balance of Automation vs Augmentation:**  \n",
    "AI offers first attempt at help, but escalates to live tutor if confidence low\n",
    "\n",
    "- **Interaction Policy:**  \n",
    "If uncertain, AI suggests: *\"Would you like a human tutor to check this?\"*\n",
    "\n",
    "- **Definition of Success:**  \n",
    "Student confidence increases; fewer escalations over time\n",
    "\n",
    "- **Measurement Plan:**  \n",
    "  - % of resolved questions without escalation  \n",
    "  - Trust rating (collected via short UI prompt)  \n",
    "  - Time saved per session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3fea7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Chapter 1 emphasizes defining user problems before building AI systems.\n",
    "- AI should augment human users unless automation adds clear value.\n",
    "- Interaction design policies prepare systems for real-world ambiguity.\n",
    "- Success is defined by **user outcomes**, not technical metrics alone."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
