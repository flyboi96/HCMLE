{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828c06e2",
   "metadata": {},
   "source": [
    "# Day 5 – Refined Evaluation Plan + PAIR Codelab\n",
    "\n",
    "## 1. Objective\n",
    "Extend evaluation criteria to include fairness, trust, and subgroup alignment. Optional: complete PAIR Codelab on building trusted AI systems.\n",
    "\n",
    "## 2. Extended Evaluation Criteria – Human-Centered Design\n",
    "\n",
    "To move beyond technical accuracy, we expand evaluation to include **trust, fairness, and safety**. These dimensions help ensure the AI product serves real users equitably and transparently.\n",
    "\n",
    "### Trust / Perception\n",
    "\n",
    "- Log **user-facing feedback** (e.g., 1–5 star helpfulness or clarity ratings)\n",
    "- Track **user override behavior** (e.g., do users ignore or correct the AI?)\n",
    "- Monitor **retention and drop-off** patterns over time\n",
    "- Detect erosion of trust by logging repeated questions or failed resolution chains\n",
    "\n",
    "### Fairness / Subgroup Alignment\n",
    "\n",
    "- Disaggregate performance metrics across **user segments**:\n",
    "  - Skill level (e.g., novice vs advanced)\n",
    "  - Language background (e.g., ESL vs native speaker)\n",
    "  - Device context (e.g., mobile vs desktop)\n",
    "  - Gender, age, or region (if known and ethically sourced)\n",
    "- Define thresholds for **performance gaps** across groups\n",
    "- Include **manual audits** or flagged edge cases from underperforming segments\n",
    "\n",
    "### Safety & Recovery\n",
    "\n",
    "- Log **confidently wrong answers** as a high-risk failure mode\n",
    "- Measure **escalation effectiveness** (e.g., how often tutor fallback resolves the issue)\n",
    "- Add **recovery metrics**:\n",
    "  - % of failures that trigger helpful fallback\n",
    "  - % of recovery attempts that result in successful resolution\n",
    "- Reward low-risk uncertainty (e.g., deferring when unsure)\n",
    "\n",
    "These dimensions make the evaluation process **reflect real human use**, not just technical model performance. This helps Tangent (AI tutor) deliver trustable, fair, and supportive learning experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fcd972",
   "metadata": {},
   "source": [
    "## Refined Evaluation Matrix – Tangent\n",
    "\n",
    "| Dimension     | Metric                              | Method of Collection     | Frequency     |\n",
    "|---------------|--------------------------------------|---------------------------|---------------|\n",
    "| Accuracy       | % correct solutions                  | System logs               | Per problem   |\n",
    "| Trust          | User helpfulness score (1–5)         | UI rating prompt          | Per session   |\n",
    "| Fairness       | Accuracy by user segment             | Segment audit             | Weekly/Monthly|\n",
    "| Safety         | % confidently wrong answers          | Error log inspection      | Per batch     |\n",
    "| Recovery       | % of escalated sessions resolved     | Escalation flow logs      | Per session   |\n",
    "| Confidence     | Avg. model certainty score           | Model output metadata     | Per response  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed511f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Extended evaluation to include human-centered metrics: trust, fairness, and safety\n",
    "- Created subgroup-aware audit plan for fairness"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
