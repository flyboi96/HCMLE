# Block 7 Reflection: Deep Learning vs Classical ML

## Key Differences Experienced

- **Feature Handling**: In classical machine learning, handling categorical variables typically required explicit feature engineering steps such as one-hot encoding, which could lead to high-dimensional sparse representations and increased memory usage. In contrast, deep learning models leveraged embedding layers that transformed categorical variables into dense, low-dimensional continuous vectors. This not only reduced dimensionality but also allowed the model to learn meaningful relationships between categories during training. Embeddings provided a more flexible and efficient way to represent categorical data, eliminating the need for manual encoding and enabling the model to capture complex patterns that classical methods might miss.

- **Model Training**: Training classical models like Random Forests or Gradient Boosting was straightforward, often requiring minimal hyperparameter tuning and providing stable performance out of the box. Deep learning, however, introduced additional complexity and potential instability. For example, I encountered challenges such as selecting an appropriate learning rate and batch size to avoid issues like vanishing gradients or divergence during training. Model diagnostics also differed; while classical models offered clear metrics like feature importance and out-of-bag error, deep learning required monitoring training and validation losses, using learning rate finders, and analyzing metrics like accuracy or AUC across epochs to detect overfitting or underfitting. This added layer of complexity demanded a deeper understanding of the training dynamics.

- **Interpretability**: Classical ML models allowed relatively straightforward interpretation through feature importance scores and visualization of decision trees. In deep learning, interpreting model decisions was more challenging. Generating meaningful SHAP values required careful consideration of the model architecture and input preprocessing, as embeddings complicated the attribution of importance to original features. Additionally, tools like Principal Component Analysis (PCA) became essential for visualizing and understanding the structure of learned embeddings, helping to reveal clusters or relationships among categories that the model had discovered. These interpretability tools were crucial for gaining insights but required more effort and expertise compared to classical methods.

- **Documentation & Workflow**: The fastai library abstracted many steps in the modeling pipeline, which both accelerated development and introduced some opacity. For instance, `TabularPandas` streamlined data preprocessing by handling categorical encoding, normalization, and splitting internally, while `DataLoaders` managed batching and shuffling seamlessly. The method `learn.get_preds` allowed easy extraction of predictions for evaluation and interpretation, but understanding the transformations applied behind the scenes required familiarity with fastai’s conventions. While these abstractions enhanced productivity, they also meant that I needed to dig deeper into the library’s internals to fully grasp how data flowed through the model and how different components interacted.

- **Compute Efficiency**: From my experiments, classical models like Random Forests and Gradient Boosting trained significantly faster on CPU, often completing within seconds to minutes depending on dataset size. Deep learning models, especially those with multiple layers and embeddings, required longer runtimes and benefited greatly from GPU acceleration to achieve practical training times. For example, training a neural network on the same tabular dataset took several minutes on GPU and much longer on CPU, highlighting the importance of hardware choice. This difference in compute efficiency underscored the trade-off between model complexity and resource requirements, with deep learning demanding more computational power but offering greater modeling flexibility.