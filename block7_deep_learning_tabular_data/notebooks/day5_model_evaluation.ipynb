{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2130cc5f",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model and Validation Data\n",
    "\n",
    "We begin Day 5 by loading the trained `Learner` object from Day 4. This allows us to evaluate model predictions on the validation set without retraining. We'll also extract the validation DataLoader to visualize inputs and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from fastai.tabular.all import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the trained Learner\n",
    "learn = load_learner('../models/block7_model.pkl')\n",
    "\n",
    "# Preview model and DataLoaders\n",
    "print(learn.model)\n",
    "print(learn.dls)\n",
    "\n",
    "# Grab validation DataLoader\n",
    "valid_dl = learn.dls.valid\n",
    "\n",
    "# Confirm number of validation items\n",
    "len(valid_dl.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db3ae1",
   "metadata": {},
   "source": [
    "## 2. Model Architecture and Validation Set Status\n",
    "\n",
    "The loaded model is a `TabularModel` trained on the Rossmann dataset to predict daily store sales. Here's a breakdown of its key architectural components:\n",
    "\n",
    "### üî¢ Embedding Layers\n",
    "The model uses **19 embedding layers** corresponding to the 19 categorical variables. Each embedding is sized based on the number of unique categories using this formula:\n",
    "\n",
    "**embedding dimension = min(50, round(1.6 √ó (num_categories)^0.56))**\n",
    "\n",
    "Examples:\n",
    "- `Store`: 1116 categories ‚Üí Embedding(1116, 81)\n",
    "- `DayOfWeek`: 8 categories ‚Üí Embedding(8, 5)\n",
    "- Many features with 3 categories ‚Üí Embedding(3, 3)\n",
    "\n",
    "These embeddings help the model learn efficient vector representations of categorical variables.\n",
    "\n",
    "### ‚öôÔ∏è Fully Connected Layers\n",
    "After processing the categorical and continuous variables, the model includes:\n",
    "- Dense layer: 172 inputs ‚Üí 200 units (with BatchNorm + ReLU)\n",
    "- Dense layer: 200 ‚Üí 100 units (with BatchNorm + ReLU)\n",
    "- Output layer: 100 ‚Üí 1 (with a `SigmoidRange(0, 41551)` to constrain outputs within realistic sales range)\n",
    "\n",
    "### üß™ Continuous Variables\n",
    "The model processes 7 continuous variables using a `BatchNorm1d(7)` layer for normalization.\n",
    "\n",
    "### üìâ Validation Set Status\n",
    "Although the validation DataLoader (`valid_dl`) exists, it appears to be empty (`len(valid_dl.items) == 0`). This likely happened because:\n",
    "- The validation split wasn‚Äôt preserved correctly during saving.\n",
    "- The underlying `TabularPandas` object was not exported or reattached.\n",
    "\n",
    "We'll address this in the next step by reloading the data and confirming the validation structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and preprocess from scratch\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "# Suppress chained assignment warnings from Pandas\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Load and merge datasets\n",
    "path = Path('../data/rossmann')\n",
    "df = pd.read_csv(path/'train.csv', low_memory=False)\n",
    "store_df = pd.read_csv(path/'store.csv')\n",
    "df = pd.merge(df, store_df, how='left', on='Store')\n",
    "\n",
    "# Feature engineering\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "add_datepart(df, 'Date', drop=True)\n",
    "\n",
    "# Define target and features\n",
    "dep_var = 'Sales'\n",
    "cat_names = ['Store', 'DayOfWeek', 'StateHoliday', 'SchoolHoliday', 'StoreType', \n",
    "             'Assortment', 'Promo', 'Promo2', 'PromoInterval', 'Month', 'Day', 'Year', 'Week', 'Dayofweek']\n",
    "cont_names = ['Customers', 'Open', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "              'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "splits = RandomSplitter(seed=42)(df)\n",
    "\n",
    "# Create TabularPandas and DataLoaders\n",
    "to = TabularPandas(df, procs=procs, cat_names=cat_names,\n",
    "                   cont_names=cont_names, y_names=dep_var, splits=splits)\n",
    "dls = to.dataloaders(bs=64)\n",
    "\n",
    "# Preview one batch\n",
    "dls.show_batch(max_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model again (if not already in memory)\n",
    "learn = load_learner('../models/block7_model.pkl')\n",
    "\n",
    "# Replace the model's DataLoaders with the new ones\n",
    "learn.dls = dls\n",
    "\n",
    "# Get validation DataLoader and items\n",
    "valid_dl = dls.valid\n",
    "\n",
    "# Get predictions and actuals\n",
    "preds, targs = learn.get_preds(dl=valid_dl)\n",
    "\n",
    "# Combine into a DataFrame for analysis\n",
    "results = pd.DataFrame({\n",
    "    'Predicted': preds.squeeze(-1).numpy(),\n",
    "    'Actual': targs.squeeze(-1).numpy()\n",
    "})\n",
    "\n",
    "# Preview top 10 rows\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f905756c",
   "metadata": {},
   "source": [
    "## 4. Visualizing Prediction Error\n",
    "\n",
    "To better understand model performance, we plot:\n",
    "- **Residuals**: the difference between predicted and actual sales.\n",
    "- **Distribution of errors** to identify bias or outliers.\n",
    "\n",
    "This helps diagnose underfitting, overfitting, or data noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b3a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add residuals to the results\n",
    "results['Residual'] = results['Predicted'] - results['Actual']\n",
    "\n",
    "# Scatter plot: Actual vs Predicted\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='Actual', y='Predicted', data=results.sample(1000))\n",
    "plt.title('Predicted vs Actual Sales')\n",
    "plt.xlabel('Actual Sales')\n",
    "plt.ylabel('Predicted Sales')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Residual histogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(results['Residual'], kde=True, bins=50, color='orange')\n",
    "plt.title('Distribution of Prediction Errors (Residuals)')\n",
    "plt.xlabel('Residual (Predicted - Actual)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d3406",
   "metadata": {},
   "source": [
    "## Evaluation: Model Predictions vs Actuals\n",
    "\n",
    "### Predicted vs Actual Sales\n",
    "\n",
    "This scatter plot compares the model's predicted sales to the actual sales values from the validation set. A perfect model would place all points along the 45¬∞ diagonal line (where predicted = actual). Key observations:\n",
    "\n",
    "- **Strong linear alignment**: Most points fall close to the diagonal, suggesting that the model has learned the general sales pattern well.\n",
    "- **Heteroskedasticity**: There is more variance in prediction error as the actual sales values increase ‚Äî likely due to more volatile sales dynamics in higher-revenue stores.\n",
    "- **Some outliers**: A few points are distant from the line, indicating cases where the model under- or over-predicted substantially. These might be due to special promotions, closed stores, or holidays not fully captured by the feature set.\n",
    "\n",
    "---\n",
    "\n",
    "### Residual Distribution (Predicted - Actual)\n",
    "\n",
    "This histogram visualizes the distribution of prediction errors (residuals):\n",
    "\n",
    "- **Tightly centered**: The bulk of residuals are concentrated around 0, indicating most predictions are close to actual values.\n",
    "- **Slight negative skew**: The tail extends further into negative territory, meaning the model occasionally under-predicts more severely than it over-predicts.\n",
    "- **Few extreme outliers**: A small number of large errors exist, which may benefit from further analysis (e.g., those few zero-prediction cases for high-sales days).\n",
    "\n",
    "---\n",
    "\n",
    "## Takeaway\n",
    "\n",
    "The model performs well overall, with tight residuals and strong correlation between predictions and targets. Focus areas for improvement may include better handling of:\n",
    "- edge cases (e.g., stores with erratic sales),\n",
    "- unmodeled promotions,\n",
    "- and days with very high or very low traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b4412",
   "metadata": {},
   "source": [
    "## Error Analysis: Under- and Over-Predictions\n",
    "\n",
    "To better understand where our model struggles, we analyze the largest **residuals** ‚Äî the difference between predicted and actual sales.\n",
    "\n",
    "- **Under-predictions**: The model predicted significantly less than the true sales.\n",
    "- **Over-predictions**: The model predicted significantly more than the true sales.\n",
    "\n",
    "By examining these edge cases, we aim to:\n",
    "- Detect patterns or conditions where the model fails\n",
    "- Uncover potential data issues (e.g., anomalies, missing values)\n",
    "- Reveal opportunities for adding features or refining preprocessing\n",
    "\n",
    "We'll sort predictions by residual magnitude and display the top 5 extreme cases in each direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58dffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add residuals to the results DataFrame\n",
    "results['Residual'] = results['Predicted'] - results['Actual']\n",
    "results['Absolute Error'] = results['Residual'].abs()\n",
    "\n",
    "# View largest under-predictions\n",
    "print(\"Top 5 Under-predictions:\")\n",
    "display(results.sort_values('Residual').head(5))\n",
    "\n",
    "# View largest over-predictions\n",
    "print(\"\\nTop 5 Over-predictions:\")\n",
    "display(results.sort_values('Residual', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faccf61",
   "metadata": {},
   "source": [
    "### Insights from Top Errors\n",
    "\n",
    "The top 5 **under-predictions** show the model significantly underestimated high-sales days, missing by as much as ~26,700 units. This may suggest:\n",
    "\n",
    "- The model failed to capture strong promotional effects or special events.\n",
    "- There might be rare patterns not well represented in the training data.\n",
    "\n",
    "The top 5 **over-predictions** had smaller absolute errors (~2,800‚Äì3,600 units), typically where the model anticipated more activity than actually occurred. Possible reasons include:\n",
    "\n",
    "- Stores expected to be open or busy (based on features) were actually closed or underperforming.\n",
    "- Categorical variables (e.g., Promo, Open) may have missing or misaligned signals.\n",
    "\n",
    "These discrepancies suggest a need for:\n",
    "- Better handling of zero sales days (possibly due to store closures).\n",
    "- Incorporating calendar-based features (e.g., holidays, weekends).\n",
    "- Additional feature engineering around promotions and competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11c47c",
   "metadata": {},
   "source": [
    "## Merging Top Errors with Original Features\n",
    "\n",
    "To better understand the model's failure modes, we examine the 10 rows with the highest absolute residuals. By joining these predictions back with the original tabular features, we can identify patterns ‚Äî such as missed promotions, store types, or date-based anomalies ‚Äî that may explain poor predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute absolute error\n",
    "results['Abs_Error'] = results['Residual'].abs()\n",
    "\n",
    "# Get top 10 rows with highest absolute error\n",
    "top_errors = results.sort_values(by='Abs_Error', ascending=False).head(10)\n",
    "\n",
    "# Use their positional indices to pull corresponding feature rows\n",
    "top_feature_rows = to.items.iloc[top_errors.index]\n",
    "\n",
    "# Combine features and prediction info\n",
    "top_error_context = top_feature_rows.copy()\n",
    "top_error_context['Predicted'] = top_errors['Predicted'].values\n",
    "top_error_context['Actual'] = top_errors['Actual'].values\n",
    "top_error_context['Residual'] = top_errors['Residual'].values\n",
    "top_error_context['Abs_Error'] = top_errors['Abs_Error'].values\n",
    "\n",
    "# Show features plus predictions\n",
    "top_error_context[['Predicted', 'Actual', 'Residual', 'Abs_Error'] + cat_names + cont_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6ef29",
   "metadata": {},
   "source": [
    "## Summary of Top Prediction Errors\n",
    "\n",
    "We now analyze patterns in the top 10 most incorrect predictions:\n",
    "- Which categorical variables show up disproportionately?\n",
    "- Are numeric inputs like `Customers` or `CompetitionDistance` unusually high/low?\n",
    "- Are there missing values in key fields?\n",
    "This helps surface potential blind spots in model generalization or training data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ef136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Categorical frequency summary\n",
    "print(\"Categorical Breakdown:\")\n",
    "for col in cat_names:\n",
    "    print(f\"{col}: {top_error_context[col].value_counts().to_dict()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. Continuous value summary\n",
    "print(\"Continuous Summary:\")\n",
    "display(top_error_context[cont_names].describe())\n",
    "\n",
    "# 3. Check missing values\n",
    "print(\"Missing Value Count (Top Errors):\")\n",
    "print(top_error_context[cat_names + cont_names].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd66b6",
   "metadata": {},
   "source": [
    "## Categorical Feature Breakdown: Top Errors vs Validation Set\n",
    "\n",
    "We analyze categorical feature distributions in the top 10 worst prediction rows compared to the full validation set.\n",
    "\n",
    "### Key Observations:\n",
    "- **Store**: Top errors come from stores with only 1‚Äì2 samples each; model likely struggles to generalize to rare stores.\n",
    "- **StateHoliday & SchoolHoliday**: All top errors occurred on rare holidays. These conditions are uncommon in the full dataset, and the model may not learn patterns for these edge cases.\n",
    "- **Promo & Promo2**: Mixed values in top errors. Since both `Promo` and `Promo2` interact with timing and store type, mispredictions may stem from compound effects not captured well by the model.\n",
    "- **Missing Value Flags** (e.g. `Promo2SinceWeek_na`): Top errors often involve missing promotion metadata, suggesting that imputation or masking might not be robust enough.\n",
    "\n",
    "This comparison highlights feature-level conditions under which the model fails most dramatically. It suggests further attention to underrepresented categorical combinations and missing-data strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e43bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of categorical frequencies: top 10 vs validation set\n",
    "\n",
    "# Extract top error rows (already sorted)\n",
    "top_errors = results.sort_values('Absolute Error', ascending=False).head(10)\n",
    "top_idx = top_errors.index\n",
    "\n",
    "# Join with original validation features\n",
    "val_x = to.items.iloc[splits[1]].reset_index(drop=True)\n",
    "top_x = val_x.iloc[top_idx]\n",
    "\n",
    "# Categorical columns only\n",
    "cat_cols = to.cat_names\n",
    "\n",
    "# Frequency breakdown for top errors\n",
    "print(\"Top 10 Errors - Categorical Frequencies:\")\n",
    "for col in cat_cols:\n",
    "    print(f\"{col}: {top_x[col].value_counts().to_dict()}\")\n",
    "\n",
    "# Compare against the full validation set (optional summary)\n",
    "print(\"\\nValidation Set - Categorical Sample Frequencies:\")\n",
    "for col in cat_cols:\n",
    "    print(f\"{col}: {val_x[col].value_counts().head(5).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae70f80",
   "metadata": {},
   "source": [
    "## Categorical Distribution Comparison: Top Errors vs Validation Set\n",
    "\n",
    "We compared the categorical values in the top 10 largest prediction errors against the overall validation set.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Rare Stores Drive High Error**: All top 10 error rows came from stores with <3 examples, such as Store 334, 114, or 707. In contrast, the most common stores in the validation set appear over 200 times. The model likely struggles to generalize to these underrepresented stores.\n",
    "\n",
    "- **Holiday Anomalies**: Every top error occurred on a `StateHoliday` (value 1) and `SchoolHoliday` (value 1), which make up only ~1% of the validation set. This suggests that holiday behavior is difficult for the model to predict accurately.\n",
    "\n",
    "- **Promo2 Metadata Missing**: The `Promo2SinceWeek_na` and `Promo2SinceYear_na` flags were present in half of the top errors. These features are missing in 49% of the validation set overall, suggesting the model may not handle their absence robustly.\n",
    "\n",
    "- **StoreType and Assortment**: The top errors are split across multiple types, but there is a slight bias toward uncommon combinations (e.g. `StoreType=3`, `Assortment=3`) which are underrepresented in the validation set.\n",
    "\n",
    "This analysis emphasizes the importance of:\n",
    "- Improving generalization to low-sample categories (e.g. store ID)\n",
    "- Enhancing feature engineering or handling of missing promo metadata\n",
    "- Potentially separating holiday models or using holiday-aware features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94322443",
   "metadata": {},
   "source": [
    "### Decoding Categorical Mappings\n",
    "\n",
    "FastAI‚Äôs `Categorify` transform encodes each unique category as an integer starting from 1 (0 is reserved for missing/NaN if applicable).  \n",
    "To interpret encoded categorical values in our trained model, we must retrieve the mapping stored in `to.classes`, where each key corresponds to a categorical column and each value is a list of the original categories (in order of encoding).\n",
    "\n",
    "This allows us to translate encoded values like `1`, `2`, `3` back into their original string representations such as `'0'`, `'a'`, `'b'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d46ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original category mappings learned by Categorify\n",
    "cat_mappings = to.classes\n",
    "for cat in ['StateHoliday', 'SchoolHoliday']:\n",
    "    print(f\"\\n{cat} mapping:\")\n",
    "    for idx, val in enumerate(cat_mappings[cat]):\n",
    "        print(f\"  {idx}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28901a3",
   "metadata": {},
   "source": [
    "### Decoding Encoded Categorical Values in Error Analysis\n",
    "\n",
    "FastAI's `Categorify` transform encoded our categorical variables into integers. Here's how the encoded values correspond to the original labels:\n",
    "\n",
    "#### StateHoliday Mapping:\n",
    "- `1` ‚Üí `'0'` (No Holiday)\n",
    "- `2` ‚Üí `'a'` (Public Holiday)\n",
    "- `3` ‚Üí `'b'` (Easter Holiday)\n",
    "- `4` ‚Üí `'c'` (Christmas Holiday)\n",
    "\n",
    "#### SchoolHoliday Mapping:\n",
    "- `1` ‚Üí `'0'` (No School Holiday)\n",
    "- `2` ‚Üí `'1'` (School Holiday)\n",
    "\n",
    "#### Key Insight:\n",
    "In our top errors, we previously noted that the values for `StateHoliday` and `SchoolHoliday` were mostly `1` or `2`. With the mappings:\n",
    "- The presence of `'a'`, `'b'`, or `'c'` (encoded as 2‚Äì4) in top errors suggests that holiday types might be difficult for the model to learn ‚Äî possibly due to class imbalance or rare events.\n",
    "- Similarly, `SchoolHoliday = '1'` (encoded as 2) might co-occur with prediction errors, indicating holiday periods disrupt normal sales patterns and are hard to model accurately.\n",
    "\n",
    "These insights can help identify **temporal features** that introduce variance, and future modeling might benefit from:\n",
    "- Grouping rare holiday types\n",
    "- Adding interaction terms like `Holiday * DayOfWeek`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f64f3",
   "metadata": {},
   "source": [
    "### Are High Residuals Coming from Underrepresented Stores?\n",
    "\n",
    "To test whether stores with large prediction errors are underrepresented:\n",
    "\n",
    "1. We compute the frequency of each `Store` in the full validation set.\n",
    "2. Then we compute the frequency of each `Store` among the top-k highest residuals.\n",
    "3. Finally, we compare proportions. If a store appears **more often in top errors** than in the full validation set, this suggests the model underfit that store ‚Äî likely due to limited training data or unique local patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea6aff",
   "metadata": {},
   "source": [
    "## Are High Residuals Tied to Specific Stores?\n",
    "\n",
    "To investigate whether certain stores were overrepresented among the worst prediction errors, we compared the store distribution in the full validation set to the top 5% of samples with the highest absolute errors.\n",
    "\n",
    "The bar chart below illustrates this comparison for the top 10 overrepresented stores. Orange bars indicate how frequently each store appears in the high-error subset, while blue bars show their proportion in the overall validation set.\n",
    "\n",
    "### Key Takeaways:\n",
    "- Stores like **#616**, **#373**, and **#392** appear disproportionately often in the top-error subset.\n",
    "- This suggests these stores may exhibit data characteristics or conditions that the model struggles to capture accurately.\n",
    "- Further inspection of their promotions, holidays, or competition status may uncover contributing factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load saved TabularPandas object\n",
    "with open(Path('../data/rossmann_tabular.pkl'), 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# Recreate validation set\n",
    "splits = RandomSplitter(seed=42)(df)\n",
    "valid_df = df.iloc[splits[1]]\n",
    "valid_df = valid_df.items.copy()  # Convert from TabularPandas to DataFrame\n",
    "\n",
    "# Compute residuals and absolute error if not yet done\n",
    "results['Residual'] = results['Predicted'] - results['Actual']\n",
    "results['abs_error'] = results['Residual'].abs()\n",
    "\n",
    "# Full validation set store frequencies\n",
    "total_store_counts = valid_df['Store'].value_counts(normalize=True)\n",
    "\n",
    "# Top 5% errors\n",
    "top_k = int(len(results) * 0.05)\n",
    "top_errors = results.nlargest(top_k, 'abs_error')\n",
    "\n",
    "# Merge results with validation features\n",
    "top_errors_full = top_errors.merge(valid_df.reset_index(), left_index=True, right_index=True)\n",
    "top_error_store_counts = top_errors_full['Store'].value_counts(normalize=True)\n",
    "\n",
    "# Comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'Overall_Proportion': total_store_counts,\n",
    "    'Top_Error_Proportion': top_error_store_counts\n",
    "}).fillna(0)\n",
    "\n",
    "comparison['Overrep'] = comparison['Top_Error_Proportion'] - comparison['Overall_Proportion']\n",
    "comparison = comparison.sort_values('Overrep', ascending=False)\n",
    "\n",
    "# Display table\n",
    "print(comparison.head(10))\n",
    "\n",
    "# Optional: visualization\n",
    "comparison.head(10)[['Overall_Proportion', 'Top_Error_Proportion']].plot(kind='bar', figsize=(12, 6))\n",
    "plt.title(\"Top 10 Overrepresented Stores in High-Error Predictions\")\n",
    "plt.ylabel(\"Proportion of Validation Samples\")\n",
    "plt.xlabel(\"Store ID\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a428d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=results, x='Actual', y='Residual', alpha=0.3)\n",
    "plt.title(\"Residuals vs. Actual Sales\")\n",
    "plt.xlabel(\"Actual Sales\")\n",
    "plt.ylabel(\"Residual (Predicted - Actual)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28aec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_day = results.copy()\n",
    "results_with_day['DayOfWeek'] = valid_df['DayOfWeek'].values\n",
    "sns.boxplot(x='DayOfWeek', y='Residual', data=results_with_day)\n",
    "plt.title(\"Residuals by Day of Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620bc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_rmse = results.join(valid_df['Store']).groupby('Store').apply(\n",
    "    lambda df: np.sqrt(np.mean((df['Residual'])**2))\n",
    ").sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133434cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cat = results.copy()\n",
    "results_cat['Promo'] = valid_df['Promo'].values\n",
    "sns.boxplot(x='Promo', y='Residual', data=results_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Predicted', y='Residual', data=results, alpha=0.3)\n",
    "plt.axhline(0, color='red', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8ec40",
   "metadata": {},
   "source": [
    "## 3. Residual Diagnostics Summary\n",
    "\n",
    "### Residuals vs. Actual Sales\n",
    "- Residual spread increases as actual sales increase, indicating **heteroscedasticity**‚Äîour model struggles more with high-sales cases.\n",
    "- A sharp downward slope at moderate sales values reflects **consistent underprediction** in a specific segment, potentially tied to promotions or rare store patterns.\n",
    "\n",
    "### Residuals by Day of Week\n",
    "- Median residuals remain close to zero across weekdays, indicating no gross model bias.\n",
    "- **Days 2 and 4 (Tuesday and Thursday)** show larger variability, possibly from operational differences like promotion launches or stocking cycles.\n",
    "\n",
    "### Residuals by Promo\n",
    "- Errors are larger during promotional periods, suggesting the model may **undervalue the uplift caused by promotions**.\n",
    "- This points to potential missed interactions between `Promo` and other store-level/contextual features.\n",
    "\n",
    "### Residuals vs. Predicted Sales\n",
    "- Most residuals cluster near zero, especially for mid-range predictions.\n",
    "- A significant underprediction cluster at low predicted values hints at **systematic failure to recognize high-sales conditions** in some edge cases.\n",
    "- The rare large residuals (e.g., <-25K) may be due to **missing competition data**, misclassified holidays, or rare categorical combinations.\n",
    "\n",
    "> These visualizations help identify where the model fails and can guide future improvements such as feature engineering, better handling of categorical rarity, or capturing nonlinear promo effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd476280",
   "metadata": {},
   "source": [
    "## Drop-Column Feature Importance\n",
    "\n",
    "To assess how much each feature contributes to the model's predictive performance, we use the drop-column approach:\n",
    "\n",
    "- For each feature in the dataset, we remove it from the input.\n",
    "- We retrain the model from scratch using the same training/validation split.\n",
    "- We compare the new RMSE to the baseline RMSE.\n",
    "- The difference in RMSE quantifies the **importance** of that feature:\n",
    "  - Larger RMSE increase ‚Üí more important feature.\n",
    "  - No change or RMSE decrease ‚Üí feature may be redundant or noisy.\n",
    "\n",
    "This technique is especially helpful for neural networks, which don‚Äôt expose feature importances as clearly as tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598dbdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load original TabularPandas data\n",
    "with open('../data/rossmann_tabular.pkl', 'rb') as f:\n",
    "    tp = pickle.load(f)\n",
    "\n",
    "# Recover raw DataFrame\n",
    "raw_df = tp.items.copy()\n",
    "\n",
    "# Recreate splits (same seed to match original)\n",
    "splits = RandomSplitter(seed=42)(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix splits format\n",
    "splits = list(map(list, splits))\n",
    "\n",
    "# Define procs from scratch\n",
    "original_procs = [Categorify, FillMissing, Normalize]\n",
    "original_cat = df.cat_names\n",
    "original_cont = df.cont_names\n",
    "target = df.y_names\n",
    "\n",
    "# Use raw_df for unprocessed input\n",
    "to_base = TabularPandas(raw_df, procs=original_procs, cat_names=original_cat, cont_names=original_cont,\n",
    "                        y_names=target, splits=splits)\n",
    "\n",
    "# Loaders and model\n",
    "dls_base = to_base.dataloaders()\n",
    "learn_base = tabular_learner(dls_base, metrics=rmse)\n",
    "learn_base.fit(1)\n",
    "\n",
    "# Baseline RMSE\n",
    "preds_base, targs_base = learn_base.get_preds()\n",
    "baseline_rmse = rmse(preds_base, targs_base).item()\n",
    "print(f\"Baseline RMSE: {baseline_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "importances = {}\n",
    "\n",
    "for feature in original_cat + original_cont:\n",
    "    # Drop the feature\n",
    "    new_cat = [f for f in original_cat if f != feature]\n",
    "    new_cont = [f for f in original_cont if f != feature]\n",
    "\n",
    "    # Create new TabularPandas without the feature\n",
    "    to_dropped = TabularPandas(raw_df, procs=original_procs, cat_names=new_cat, cont_names=new_cont,\n",
    "                                y_names=target, splits=splits)\n",
    "    \n",
    "    dls_dropped = to_dropped.dataloaders()\n",
    "    learn_dropped = tabular_learner(dls_dropped, metrics=rmse)\n",
    "    learn_dropped.fit(1)\n",
    "\n",
    "    preds_dropped, targs_dropped = learn_dropped.get_preds()\n",
    "    dropped_rmse = rmse(preds_dropped, targs_dropped).item()\n",
    "\n",
    "    # Store RMSE increase\n",
    "    importances[feature] = dropped_rmse - baseline_rmse\n",
    "    print(f\"{feature}: ŒîRMSE = {dropped_rmse - baseline_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e12e2",
   "metadata": {},
   "source": [
    "## Feature Importance Results\n",
    "\n",
    "The following bar chart displays the RMSE increase when each feature is dropped from the model. Features with the highest positive ŒîRMSE are the most important‚Äîmeaning the model relies heavily on them to make accurate predictions.\n",
    "\n",
    "A negative or near-zero ŒîRMSE suggests:\n",
    "- The feature may be redundant with other features.\n",
    "- It may introduce noise or confusion in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e519cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort by importance (descending)\n",
    "sorted_importances = dict(sorted(importances.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(list(sorted_importances.keys()), list(sorted_importances.values()))\n",
    "plt.xlabel(\"Œî RMSE when Dropped\")\n",
    "plt.title(\"Drop-Column Feature Importances\")\n",
    "plt.gca().invert_yaxis()  # Most important at top\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f6cd0",
   "metadata": {},
   "source": [
    "## Drop-Column Feature Importance Summary\n",
    "\n",
    "The chart above quantifies the importance of each feature by measuring how much the model's RMSE increases when that feature is removed. Key observations:\n",
    "\n",
    "- **Customers** is the most critical feature by far. Dropping it causes the RMSE to increase by over **2 million**, confirming that customer traffic is essential to accurate sales predictions.\n",
    "- **CompetitionOpenSinceMonth_na** and **Assortment** also contribute significantly to prediction accuracy. Their high ŒîRMSE indicates that store-level differences in assortment and whether competition data is missing play an important role.\n",
    "- **Promo2** and **Promo** features are highly informative, reinforcing the importance of understanding promotional strategies in retail forecasting.\n",
    "- **StoreType** and **PromoInterval** also contribute meaningfully, while many calendar-based variables (e.g., `Year`, `DayOfWeek`, `Week`, `Month`) show minimal impact on performance.\n",
    "- Several features like `Dayofweek`, `Promo2SinceYear`, `SchoolHoliday`, and `StateHoliday` contributed little or no value, suggesting they may be redundant or less relevant to this model's architecture.\n",
    "\n",
    "This analysis helps guide future feature engineering or model simplification efforts by highlighting which inputs the model actually uses to generate accurate forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03abaa",
   "metadata": {},
   "source": [
    "## Residuals by Store Frequency Tier\n",
    "\n",
    "In this analysis, we investigate whether the model performs differently for stores that appear more or less frequently in the training data.\n",
    "\n",
    "We bin stores into three tiers based on how many training examples each store has:\n",
    "- **Low Frequency**: Bottom third (least common stores)\n",
    "- **Medium Frequency**: Middle third\n",
    "- **High Frequency**: Top third (most common stores)\n",
    "\n",
    "We then compare the average residuals across these groups on the validation set. This helps assess whether **underrepresented stores** tend to experience larger errors, potentially due to insufficient training data or overfitting to more common stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a05ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count store frequencies in training set\n",
    "train_store_counts = df.iloc[splits[0]]['Store'].value_counts()\n",
    "\n",
    "# Bin stores into tiers by frequency rank\n",
    "store_bins = pd.qcut(train_store_counts, 3, labels=['Low Frequency', 'Medium Frequency', 'High Frequency'])\n",
    "store_tiers = store_bins.to_dict()\n",
    "\n",
    "# Map tier labels onto validation DataFrame\n",
    "valid_df['Store_Tier'] = valid_df['Store'].map(store_tiers)\n",
    "\n",
    "# Merge residuals with validation set\n",
    "residuals_with_tiers = results.copy()\n",
    "residuals_with_tiers['Residual'] = residuals_with_tiers['Residual']\n",
    "residuals_with_tiers['Store_Tier'] = valid_df['Store'].map(store_tiers)\n",
    "\n",
    "# Drop any rows with missing tier labels (e.g., unseen stores)\n",
    "residuals_with_tiers = residuals_with_tiers.dropna(subset=['Store_Tier'])\n",
    "\n",
    "# Visualize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=residuals_with_tiers, x='Store_Tier', y='Residual')\n",
    "plt.axhline(0, linestyle='--', color='gray')\n",
    "plt.title(\"Residuals by Store Frequency Tier\")\n",
    "plt.ylabel(\"Residual (Predicted - Actual)\")\n",
    "plt.xlabel(\"Store Frequency Tier\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d62b2",
   "metadata": {},
   "source": [
    "### Residuals by Store Frequency Tier\n",
    "\n",
    "This plot compares the residuals of predictions across three store frequency tiers‚Äî**High**, **Medium**, and **Low**‚Äîbased on how often each store appeared in the training set.\n",
    "\n",
    "**Observations:**\n",
    "- **Median residuals** are fairly consistent across tiers, indicating that the model is not systematically biased toward under- or over-predicting for any specific group.\n",
    "- However, **low-frequency stores** exhibit slightly wider variance and more extreme negative residuals, suggesting the model may perform less reliably for underrepresented stores.\n",
    "- **High-frequency stores** generally have tighter interquartile ranges and fewer severe outliers, consistent with the idea that more training data improves prediction stability.\n",
    "\n",
    "This reinforces earlier findings that rare stores are more likely to appear in the top error group and should be treated with special care in production (e.g., via data augmentation or store-specific fine-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc930f2",
   "metadata": {},
   "source": [
    "### Temporal Drift Analysis\n",
    "\n",
    "To check for signs of temporal drift ‚Äî where model performance degrades over time ‚Äî we analyze prediction residuals grouped by time-based features. This helps identify if our model systematically under- or over-predicts in certain months, years, or calendar periods. Temporal drift may indicate a need for periodic retraining or dynamic adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Merge relevant temporal features into results\n",
    "temporal_cols = ['Year', 'Month', 'Week', 'Day', 'DayOfWeek']\n",
    "results_with_time = results.merge(valid_df[temporal_cols], left_index=True, right_index=True)\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(nrows=len(temporal_cols), ncols=1, figsize=(8, 4 * len(temporal_cols)))\n",
    "\n",
    "if len(temporal_cols) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, col in zip(axes, temporal_cols):\n",
    "    sns.boxplot(x=results_with_time[col], y=results_with_time['Residual'], ax=ax)\n",
    "    ax.set_title(f'Residuals by {col}')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.axhline(0, color='gray', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea6b69",
   "metadata": {},
   "source": [
    "### Temporal Drift Analysis ‚Äì Residuals Over Time\n",
    "\n",
    "We examined residuals across several temporal dimensions to detect any signs of performance degradation over time, which might suggest temporal drift in our model's predictions:\n",
    "\n",
    "- **Year**: Residuals appear tightly clustered within the available year(s), suggesting stable performance at the annual level.\n",
    "- **Month**: Slight variation in median residuals across months, with some months (e.g., March and June) showing wider residual spread, indicating potential seasonal underperformance.\n",
    "- **Week**: Weekly residuals are fairly stable, though specific weeks show spikes in error, which may correspond to promotions, holidays, or data gaps.\n",
    "- **Day**: Daily residual patterns show consistent variance, with no strong evidence of drift on a day-of-month basis.\n",
    "- **DayOfWeek**: Residuals by weekday reveal larger negative errors on some days (especially around midweek), potentially due to unmodeled dynamics in store activity or promotional cadence.\n",
    "\n",
    "**Conclusion**: While there is no strong evidence of long-term drift, some short-term fluctuations in prediction quality across months and weekdays may warrant further feature engineering or temporal stratification in future modeling efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea366195",
   "metadata": {},
   "source": [
    "## Calibration Curve for Regression\n",
    "\n",
    "A calibration curve assesses how well predicted values align with actual values across the range of outputs. While traditionally used in classification, we adapt the concept for regression by:\n",
    "\n",
    "- Binning predicted values into quantiles\n",
    "- Comparing the average predicted vs. actual target value in each bin\n",
    "- Ideal calibration would follow a 45¬∞ line: `Predicted = Actual`\n",
    "\n",
    "This helps us determine if our model tends to systematically **overpredict or underpredict** in certain ranges of the output space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make sure we have predictions and targets\n",
    "preds = results['Predicted']\n",
    "actuals = results['Actual']\n",
    "\n",
    "# Bin predicted values into quantiles\n",
    "bin_count = 10\n",
    "bins = pd.qcut(preds, q=bin_count, duplicates='drop')\n",
    "df_bin = pd.DataFrame({'bin': bins, 'pred': preds, 'actual': actuals})\n",
    "\n",
    "# Group by bin and compute mean predicted and actual sales\n",
    "calibration_df = df_bin.groupby('bin').agg({\n",
    "    'pred': 'mean',\n",
    "    'actual': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Plot calibration curve\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.lineplot(data=calibration_df, x='pred', y='actual', marker='o')\n",
    "plt.plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], 'k--', label='Perfect Calibration')\n",
    "plt.xlabel('Mean Predicted Sales')\n",
    "plt.ylabel('Mean Actual Sales')\n",
    "plt.title('Calibration Curve for Regression Model')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15740d05",
   "metadata": {},
   "source": [
    "## Calibration Curve Analysis\n",
    "\n",
    "The calibration curve assesses how well the predicted sales values align with the actual observed values. Each point represents a bin of predictions (equal-sized groups), and its position indicates the average actual sales versus average predicted sales in that bin.\n",
    "\n",
    "### Key Observations:\n",
    "- The curve closely tracks the diagonal \"perfect calibration\" line, suggesting the model is **well-calibrated** across most of the prediction range.\n",
    "- Minor deviations from the line at higher predicted sales may indicate **slight underprediction** for top-tier sales values.\n",
    "- The overall consistency implies the model's output can be **interpreted directly as meaningful expected values**, which is especially useful in business forecasting contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc6db04",
   "metadata": {},
   "source": [
    "## Underperforming Store Clusters\n",
    "\n",
    "In this analysis, we aim to identify groups of stores that consistently yield higher residual errors, potentially indicating systematic underperformance. By grouping residuals at the store level, we can detect clusters where the model struggles to predict accurately. These insights can help uncover:\n",
    "\n",
    "- Regional, operational, or promotional inefficiencies.\n",
    "- Potential data quality issues.\n",
    "- Opportunities to segment or fine-tune the model for specific subsets.\n",
    "\n",
    "We calculate the mean absolute error (MAE) per store and visualize the distribution of error across stores. Clustering techniques (e.g., k-means) may be applied to identify patterns among poorly performing stores based on available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ead757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TabularPandas to a standard DataFrame\n",
    "valid_df_df = valid_df.items if hasattr(valid_df, 'items') else valid_df.to_df()\n",
    "\n",
    "# Merge residuals with validation data to access 'Store' column\n",
    "results_with_store = results.merge(valid_df_df[['Store']], left_index=True, right_index=True)\n",
    "results_with_store['abs_error'] = results_with_store['Residual'].abs()\n",
    "\n",
    "# Calculate MAE per store\n",
    "store_mae = results_with_store.groupby('Store')['abs_error'].mean().reset_index()\n",
    "store_mae = store_mae.rename(columns={'abs_error': 'Store_MAE'})\n",
    "\n",
    "# Get one row per store to extract its features\n",
    "store_features = valid_df_df.groupby('Store').first().reset_index()\n",
    "\n",
    "# Merge MAE values with features\n",
    "store_analysis = pd.merge(store_mae, store_features, on='Store')\n",
    "\n",
    "# Plot histogram of store MAEs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(store_analysis['Store_MAE'], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Mean Absolute Error (MAE) Across Stores\")\n",
    "plt.xlabel(\"Mean Absolute Error\")\n",
    "plt.ylabel(\"Number of Stores\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show top 20 underperforming stores\n",
    "top_n = 20\n",
    "worst_stores = store_analysis.sort_values('Store_MAE', ascending=False).head(top_n)\n",
    "print(\"Top Underperforming Stores by MAE:\")\n",
    "display(worst_stores[['Store', 'Store_MAE']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325ae2b",
   "metadata": {},
   "source": [
    "## Residual Analysis: Underperforming Store Clusters\n",
    "\n",
    "To investigate whether specific stores consistently perform worse, we calculated the **Mean Absolute Error (MAE)** per store. This gives us a store-level view of prediction accuracy, highlighting outliers that may indicate systemic underperformance.\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- The histogram shows a right-skewed distribution of store-level MAE, with the majority of stores clustered between 250 and 400 units of error.\n",
    "- However, a small set of stores exhibit **significantly higher MAEs**, some exceeding 800, suggesting the model struggles to predict their sales accurately.\n",
    "- These underperforming stores may be:\n",
    "  - Outliers in behavior (e.g., unusual promo patterns, irregular hours)\n",
    "  - Poorly represented in the training data\n",
    "  - Impacted by missing or mis-encoded features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4443fc61",
   "metadata": {},
   "source": [
    "## Residual Distribution by Assortment and Store Type\n",
    "\n",
    "To identify whether model errors are systematically related to store characteristics, we examine the distribution of residuals across different `Assortment` and `StoreType` categories.\n",
    "\n",
    "These categorical features define key operational characteristics of a store:\n",
    "- **Assortment** refers to the breadth of products offered (e.g., basic, extended).\n",
    "- **StoreType** refers to store format or ownership structure.\n",
    "\n",
    "By plotting residuals for each group, we can identify whether certain store types or assortment classes are prone to systematic over- or under-predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Merge residuals with validation metadata\n",
    "residuals_with_meta = results.join(valid_df[['Assortment', 'StoreType']], how='left')\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residuals by Assortment\n",
    "sns.boxplot(data=residuals_with_meta, x='Assortment', y='Residual', ax=axes[0])\n",
    "axes[0].axhline(0, linestyle='--', color='gray')\n",
    "axes[0].set_title('Residuals by Assortment')\n",
    "axes[0].set_ylabel('Residual')\n",
    "axes[0].set_xlabel('Assortment')\n",
    "\n",
    "# Residuals by StoreType\n",
    "sns.boxplot(data=residuals_with_meta, x='StoreType', y='Residual', ax=axes[1])\n",
    "axes[1].axhline(0, linestyle='--', color='gray')\n",
    "axes[1].set_title('Residuals by StoreType')\n",
    "axes[1].set_ylabel('Residual')\n",
    "axes[1].set_xlabel('Store Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d04238",
   "metadata": {},
   "source": [
    "## Analysis: Residual Distribution by Assortment and Store Type\n",
    "\n",
    "The residual distribution plots above reveal how prediction errors vary across categorical store features:\n",
    "\n",
    "- **Assortment Types**:\n",
    "  - Assortment levels represent the product breadth offered by the store:\n",
    "    - `1`: Basic\n",
    "    - `2`: Extra\n",
    "    - `3`: Extended\n",
    "  - All three levels show residuals generally centered around zero, but **Assortment 1 (Basic)** has heavier negative outliers. This suggests that the model may underpredict sales more frequently for stores with a limited assortment.\n",
    "\n",
    "- **Store Types**:\n",
    "  - StoreType indicates the operational format or business model:\n",
    "    - `1`: Type a\n",
    "    - `2`: Type b\n",
    "    - `3`: Type c\n",
    "    - `4`: Type d\n",
    "  - Residual distributions are broadly consistent across types, but **StoreType 4 (Type d)** shows the largest spread and most extreme errors, implying greater variability in model performance. This may stem from operational differences not well represented in the current feature set.\n",
    "\n",
    "### Key Insight:\n",
    "The variation in residual spread across assortment and store types suggests that additional modeling attention may be needed for stores with Basic assortments and for those under StoreType 4. Including more fine-grained operational or regional attributes could help reduce systematic error in these segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd04a3",
   "metadata": {},
   "source": [
    "## Prediction Density Plot\n",
    "\n",
    "This plot compares the distribution of predicted sales to the distribution of actual sales in the validation set. It serves as a visual check for how well the model captures the overall shape and scale of the target variable.\n",
    "\n",
    "Key interpretations:\n",
    "- **Alignment of peaks** suggests the model correctly predicts common sales values.\n",
    "- **Mismatches in tails** can indicate under- or overestimation for low or high sales scenarios.\n",
    "- A well-calibrated model should have its prediction density closely mirror the true distribution of the target.\n",
    "\n",
    "This visualization complements residual analysis by focusing on **distribution fit** rather than error magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7393fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract actual and predicted sales\n",
    "actual = results['Actual']\n",
    "predicted = results['Predicted']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(actual, label='Actual Sales', fill=True, alpha=0.4)\n",
    "sns.kdeplot(predicted, label='Predicted Sales', fill=True, alpha=0.4)\n",
    "\n",
    "plt.title(\"Prediction vs. Actual Sales Density\")\n",
    "plt.xlabel(\"Sales\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a6289",
   "metadata": {},
   "source": [
    "## Prediction vs. Actual Sales Density ‚Äì Analysis\n",
    "\n",
    "The prediction density plot compares the distributions of predicted sales and actual sales:\n",
    "\n",
    "- **Overall Fit**: The predicted sales distribution closely follows the actual sales distribution, indicating that the model captures the general shape and magnitude of sales.\n",
    "- **Peak Alignment**: The two density peaks are well-aligned, suggesting strong calibration for the most common sales ranges.\n",
    "- **Left Tail (Low Sales)**: There is a slight overestimation for very low sales values, as the predicted density is marginally higher near zero. This could be due to the model predicting small positive values where actual sales were close to zero.\n",
    "- **Right Tail (High Sales)**: The model captures the long right tail reasonably well, though some high sales are slightly underrepresented in the predicted distribution.\n",
    "- **Conclusion**: The close match between predicted and actual distributions suggests that the model is well-calibrated in its outputs, with only minor areas for improvement in edge cases (e.g., very low or very high sales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bbe6fe",
   "metadata": {},
   "source": [
    "## Residual Autocorrelation Analysis\n",
    "\n",
    "To explore whether prediction errors are temporally autocorrelated, we align residuals with the original `Date` values from the training dataset. Autocorrelation in residuals can indicate unmodeled temporal structure or performance drift over time. Identifying such patterns helps guide feature engineering or inform time-aware model choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw training data to recover dates\n",
    "train_raw = pd.read_csv('../data//rossmann/train.csv', parse_dates=['Date'])\n",
    "\n",
    "# Ensure residuals align with validation indices\n",
    "val_idx = splits[1]\n",
    "dates = train_raw.iloc[val_idx]['Date'].reset_index(drop=True)\n",
    "\n",
    "# Merge with residuals\n",
    "residuals_sorted = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Residual': results['Residual'].values\n",
    "}).sort_values('Date')\n",
    "\n",
    "# Plot autocorrelation\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_acf(residuals_sorted['Residual'], lags=50)\n",
    "plt.title('Autocorrelation of Residuals')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6692eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map residuals back to dates using validation set indices\n",
    "val_idx = splits[1]\n",
    "val_dates = train_raw.iloc[val_idx]['Date'].reset_index(drop=True)\n",
    "\n",
    "# Combine residuals with their true dates\n",
    "residuals_df = pd.DataFrame({\n",
    "    'Date': val_dates,\n",
    "    'Residual': results['Residual'].values\n",
    "})\n",
    "\n",
    "# Aggregate residuals per day\n",
    "daily_residuals = residuals_df.groupby('Date')['Residual'].mean().reset_index().sort_values('Date')\n",
    "\n",
    "# Plot autocorrelation of daily-averaged residuals\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_acf(daily_residuals['Residual'], lags=50)\n",
    "plt.title('Autocorrelation of Daily-Averaged Residuals')\n",
    "plt.xlabel('Lag (days)')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881aad1b",
   "metadata": {},
   "source": [
    "## Residual Autocorrelation Analysis\n",
    "\n",
    "We examined the autocorrelation of daily-averaged residuals to assess whether the model's prediction errors show temporal dependencies. This helps identify if there are recurring patterns or cycles in the error behavior over time.\n",
    "\n",
    "### Key Findings:\n",
    "- The autocorrelation plot shows **strong, significant positive spikes at 7-day intervals** (e.g., lags at 7, 14, 21, etc.).\n",
    "- This pattern suggests the presence of **weekly cycles** in the residuals ‚Äî indicating that the model's errors are **not temporally independent**.\n",
    "- The weekly autocorrelation implies that some element of **weekly seasonality is not being fully captured** by the current model or feature set.\n",
    "\n",
    "### Implications:\n",
    "- The model may be missing:\n",
    "  - Important **calendar or lag-based features** (e.g., previous week's sales).\n",
    "  - **Interactions** between features such as promo status and day of the week.\n",
    "- This warrants exploration of:\n",
    "  - Adding **explicit time-lag features** (`Sales_t-7`, `Promo_t-7`, etc.).\n",
    "  - Encoding **seasonal effects more robustly** (e.g., one-hot encoding of `WeekOfYear` or harmonic transformations).\n",
    "  - Considering **time series models** (e.g., RNNs, transformers, or Prophet) if the temporal dependency remains persistent.\n",
    "\n",
    "This residual autocorrelation pattern is a strong indicator that temporal structure remains in the errors and could be leveraged to further improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283e055",
   "metadata": {},
   "source": [
    "# Day 5: Model Error Analysis and Interpretability\n",
    "\n",
    "## 1. Objective\n",
    "Deepen understanding of the model‚Äôs performance by examining residual patterns, error distribution across features, and feature importance using post-hoc interpretability techniques.\n",
    "\n",
    "## 2. Key Steps\n",
    "- Computed and visualized residuals for the validation set.\n",
    "- Investigated residual patterns by:\n",
    "  - Store frequency tiers\n",
    "  - Data completeness flags\n",
    "  - Temporal drift (date-based trends)\n",
    "  - Calibration curve alignment\n",
    "  - Assortment and store type levels\n",
    "  - Prediction density distribution\n",
    "- Clustered stores based on error metrics to identify underperforming groups.\n",
    "- Conducted drop-column importance to measure the marginal contribution of each feature.\n",
    "- Attempted residual autocorrelation analysis with lag features from the raw training set.\n",
    "\n",
    "## 3. Results\n",
    "- Model errors were concentrated in underrepresented stores.\n",
    "- Certain categories (e.g. rare `StoreType` or `Assortment` values) showed significantly higher residual variance.\n",
    "- Drop-column analysis revealed key drivers like `Promo`, `Store`, and `CompetitionDistance`.\n",
    "- Calibration curve suggested mild overconfidence in predictions.\n",
    "- Prediction density revealed moderate over-smoothing at distribution tails.\n",
    "- Residuals did not exhibit strong autocorrelation with `Sales_Lag1`, though further lag-based features may help.\n",
    "\n",
    "## 4. Summary\n",
    "- Performed a comprehensive residual and interpretability study to uncover hidden failure modes.\n",
    "- Identified patterns of systematic error associated with store sparsity, promo activity, and category imbalance.\n",
    "- Drop-column importance provided insights into most critical features.\n",
    "- Set up groundwork for deeper temporal modeling using lagged features or recurrent architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-tabular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
