{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9132365f",
   "metadata": {},
   "source": [
    "# Day 6: Feature Importance & Interpretability\n",
    "\n",
    "## 1. Objective\n",
    "- Interpret the trained model to understand which features are most influential\n",
    "- Use permutation-based importance to cross-check drop-column results\n",
    "- Visualize and describe feature effects and patterns\n",
    "- Prepare notes and visuals for interpretability documentation\n",
    "\n",
    "## 2. Key Steps\n",
    "- Calculate permutation feature importances\n",
    "- Visualize and compare with drop-column results\n",
    "- Optionally run SHAP on a sample (resource-dependent)\n",
    "- Group insights by feature types (categorical, continuous, engineered)\n",
    "- Prepare summary visualizations and markdown explanations\n",
    "\n",
    "## 3. Results\n",
    "- Feature importance plots and sorted rankings\n",
    "- Interpretability summary with commentary\n",
    "- Optional SHAP force plots or value distributions\n",
    "\n",
    "## 4. Summary\n",
    "_To be completed after running the notebook_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a612f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from fastai.tabular.all import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the trained Learner\n",
    "learn = load_learner('../models/block7_model.pkl')\n",
    "\n",
    "# Preview model and DataLoaders\n",
    "print(learn.model)\n",
    "print(learn.dls)\n",
    "\n",
    "# Grab validation DataLoader\n",
    "valid_dl = learn.dls.valid\n",
    "\n",
    "# Confirm number of validation items\n",
    "len(valid_dl.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and preprocess from scratch\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "# Suppress chained assignment warnings from Pandas\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Load and merge datasets\n",
    "path = Path('../data/rossmann')\n",
    "df = pd.read_csv(path/'train.csv', low_memory=False)\n",
    "store_df = pd.read_csv(path/'store.csv')\n",
    "df = pd.merge(df, store_df, how='left', on='Store')\n",
    "\n",
    "# Feature engineering\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "add_datepart(df, 'Date', drop=True)\n",
    "\n",
    "# Define target and features\n",
    "dep_var = 'Sales'\n",
    "cat_names = ['Store', 'DayOfWeek', 'StateHoliday', 'SchoolHoliday', 'StoreType', \n",
    "             'Assortment', 'Promo', 'Promo2', 'PromoInterval', 'Month', 'Day', 'Year', 'Week', 'Dayofweek']\n",
    "cont_names = ['Customers', 'Open', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "              'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "splits = RandomSplitter(seed=42)(df)\n",
    "\n",
    "# Create TabularPandas and DataLoaders\n",
    "to = TabularPandas(df, procs=procs, cat_names=cat_names,\n",
    "                   cont_names=cont_names, y_names=dep_var, splits=splits)\n",
    "dls = to.dataloaders(bs=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7624f9c3",
   "metadata": {},
   "source": [
    "## Permutation Feature Importance\n",
    "\n",
    "Permutation importance estimates how much each individual feature contributes to model performance by randomly shuffling its values and observing the drop in accuracy (RMSE in our case). Unlike drop-column importance, which retrains the model without a feature, permutation importance keeps the model fixed and perturbs the input data.\n",
    "\n",
    "This is particularly helpful when using neural networks, as it gives insight into which inputs the trained model relies on most—without requiring retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62078082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "# Save original validation set and target\n",
    "val_idx = splits[1]\n",
    "y_val = df.loc[val_idx, dep_var].values\n",
    "\n",
    "# Baseline RMSE\n",
    "base_dl = to.dataloaders(bs=64).valid\n",
    "base_preds = learn.get_preds(dl=base_dl)[0].squeeze().numpy()\n",
    "baseline_rmse = root_mean_squared_error(y_val, base_preds)\n",
    "print(f\"Baseline RMSE: {baseline_rmse:.4f}\")\n",
    "\n",
    "# Only use columns that exist in original df\n",
    "valid_cols = [col for col in (cat_names + cont_names) if col in df.columns]\n",
    "\n",
    "# Permutation importance loop\n",
    "feature_importances = []\n",
    "for i, col in enumerate(valid_cols, 1):\n",
    "    df_permuted = df.copy()\n",
    "    df_permuted.loc[val_idx, col] = np.random.permutation(df_permuted.loc[val_idx, col].values)\n",
    "\n",
    "    # Reprocess permuted data\n",
    "    to_perm = TabularPandas(df_permuted, procs=procs, cat_names=cat_names,\n",
    "                            cont_names=cont_names, y_names=dep_var, splits=splits)\n",
    "    dls_perm = to_perm.dataloaders(bs=64)\n",
    "    dl_perm = dls_perm.valid\n",
    "\n",
    "    try:\n",
    "        preds = learn.get_preds(dl=dl_perm)[0].squeeze().numpy()\n",
    "        if preds.shape != y_val.shape:\n",
    "            print(f\"[{i}/{len(cat_names + cont_names)}] {col}: skipped (bad shape)\")\n",
    "            continue\n",
    "        perm_rmse = root_mean_squared_error(y_val, preds)\n",
    "        delta = perm_rmse - baseline_rmse\n",
    "        feature_importances.append((col, delta))\n",
    "        print(f\"[{i}/{len(cat_names + cont_names)}] {col}: ΔRMSE = {delta:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{i}/{len(cat_names + cont_names)}] {col}: error - {e}\")\n",
    "        continue\n",
    "\n",
    "# Save to DataFrame\n",
    "importances_df = pd.DataFrame(feature_importances, columns=[\"Feature\", \"Importance\"])\n",
    "importances_df = importances_df.sort_values(by=\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350dd921",
   "metadata": {},
   "source": [
    "## Visualizing Permutation Feature Importance\n",
    "\n",
    "The plot below shows how much each feature contributes to the model’s performance, measured by the increase in RMSE when that feature is randomly shuffled.\n",
    "\n",
    "Features at the top are the most important — their disruption causes the largest drop in prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure plots directory exists\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# Plot top 20 features\n",
    "top_n = 20\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importances_df[\"Feature\"].head(top_n)[::-1], \n",
    "         importances_df[\"Importance\"].head(top_n)[::-1])\n",
    "plt.xlabel(\"Increase in RMSE\")\n",
    "plt.title(\"Top 20 Permutation Feature Importances\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plot_path = \"../plots/permutation_feature_importance.png\"\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff285f7",
   "metadata": {},
   "source": [
    "## 3. Permutation Feature Importance Analysis\n",
    "\n",
    "Permutation importance measures the impact of randomly shuffling each feature on the model's performance (here, RMSE). The idea is that if shuffling a feature significantly degrades model accuracy, that feature is important for predictions.\n",
    "\n",
    "### Key Observations:\n",
    "- `Customers` is by far the most important feature. Shuffling it caused a large increase in RMSE, indicating it is critical for accurate sales prediction.\n",
    "- `Store`, `Open`, and `Promo` also show strong contributions. These features directly relate to business operations and availability.\n",
    "- `StoreType`, `DayOfWeek`, and `CompetitionDistance` contribute moderately, possibly encoding customer behavior and store characteristics.\n",
    "- Features like `StateHoliday`, `SchoolHoliday`, `Promo2`, and various date parts (`Month`, `Day`, etc.) appear to have minimal impact individually—though they may contribute in interaction terms.\n",
    "\n",
    "### Takeaways:\n",
    "- The model heavily depends on a small set of intuitive features.\n",
    "- Many engineered time-based and promotional features offer minimal marginal value when considered in isolation.\n",
    "- Further dimensionality reduction or feature selection could be explored, although interactions may still carry important signals not captured by permutation alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8be380",
   "metadata": {},
   "source": [
    "## 4. Comparison: Permutation vs. Drop-Column Feature Importance\n",
    "\n",
    "Both permutation and drop-column importance aim to measure how much each feature contributes to model performance. However, they differ in how they assess this impact:\n",
    "\n",
    "### Method Differences:\n",
    "- **Drop-Column Importance**:\n",
    "  - Retrains the model from scratch without each feature.\n",
    "  - Measures performance degradation (ΔRMSE) from this retrained model.\n",
    "  - Captures impact including interactions with other features.\n",
    "- **Permutation Importance**:\n",
    "  - Keeps the model fixed.\n",
    "  - Randomly permutes one feature's values and observes prediction degradation.\n",
    "  - Faster to compute but may miss interaction effects or be noisy for correlated features.\n",
    "\n",
    "### Key Observations:\n",
    "| Feature                    | Drop-Column Rank | Permutation Rank | Notes |\n",
    "|---------------------------|------------------|------------------|-------|\n",
    "| `Customers`               | 1                | 1                | Consistently dominant |\n",
    "| `Store`                   | 20               | 2                | Much higher in permutation — model may rely on store-specific bias |\n",
    "| `Open`                    | Low              | 3                | Shows strong effect in permutation, likely due to direct logic |\n",
    "| `Promo`, `Promo2`         | High             | 4–20             | Strong in both, though more emphasized in drop-column |\n",
    "| `Assortment`              | 3                | Lower            | Higher impact when retraining model; less so when permuted |\n",
    "| `Competition*`, `Promo2Since*`, `*_na` | Top 10+ in drop-column | Very low | Likely due to learned interactions and imputation handling during training |\n",
    "\n",
    "### Takeaways:\n",
    "- `Customers` is unequivocally the most important feature by both methods.\n",
    "- `Store` and `Open` appear much more important in permutation than in drop-column, suggesting the model strongly relies on memorized patterns rather than learned generalizations.\n",
    "- Drop-column tends to reveal interaction-based importance (e.g., `CompetitionOpenSinceMonth_na`, `Assortment`) better than permutation.\n",
    "- Features like `Promo2SinceYear` and `_na` flags show importance only during training—likely due to preprocessing steps that permutation doesn’t disrupt.\n",
    "\n",
    "### Recommendation:\n",
    "Use **drop-column importance** for a more thorough and reliable understanding of feature relevance, especially when interactions or missing indicators are involved. Use **permutation importance** for quick assessments or sanity checks of trained model reliance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498b065",
   "metadata": {},
   "source": [
    "## Partial Dependence Plots (PDP)\n",
    "\n",
    "Partial Dependence Plots help us visualize the marginal effect of individual features on the predicted target value, while averaging out the influence of all other features. This is useful for interpreting how the model reacts to changes in a single feature across its range.\n",
    "\n",
    "In this analysis, we generate PDPs for the most influential features to better understand their relationship with model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. Define inputs ---\n",
    "features = ['Store', 'DayOfWeek', 'StateHoliday', 'SchoolHoliday', 'StoreType', \n",
    "            'Assortment', 'Promo', 'Promo2', 'PromoInterval', 'Month', 'Day', \n",
    "            'Year', 'Week', 'Dayofweek', 'Customers', 'Open', 'CompetitionDistance',\n",
    "            'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', \n",
    "            'Promo2SinceWeek', 'Promo2SinceYear']\n",
    "dep_var = 'Sales'\n",
    "\n",
    "# --- 2. Sample, clean, and encode ---\n",
    "df_sub = df[features + [dep_var]].sample(10000, random_state=42).dropna()\n",
    "X = pd.get_dummies(df_sub[features])\n",
    "y = df_sub[dep_var]\n",
    "\n",
    "# --- 3. Train model ---\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# --- 4. Prepare to save plots ---\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "top_features = ['Customers', 'Promo', 'DayOfWeek', 'Month', 'CompetitionDistance']\n",
    "\n",
    "# --- 5. Plot and save each PDP ---\n",
    "for feat in top_features:\n",
    "    if feat in X.columns:\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        PartialDependenceDisplay.from_estimator(model, X, [feat], ax=ax)\n",
    "        plt.title(f\"PDP: {feat}\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        filename = f\"../plots/pdp_{feat.lower().replace(' ', '_')}.png\"\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Feature {feat} not found in X.columns — skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb6415",
   "metadata": {},
   "source": [
    "## 3.3 Partial Dependence Plots (PDPs)\n",
    "\n",
    "Partial Dependence Plots (PDPs) help visualize the marginal effect of individual features on the predicted outcome. By varying one feature while holding others constant, PDPs isolate how the model prediction responds to specific inputs.\n",
    "\n",
    "We generated PDPs for the top 5 features based on their importance and interpretability:\n",
    "\n",
    "- **Customers**: Strong linear relationship with Sales. As expected, higher customer counts directly increase predicted sales.\n",
    "- **Promo**: Clear positive impact. Promotions are associated with a large increase in predicted sales, showing their effectiveness.\n",
    "- **DayOfWeek**: Significant variation across days, with weekends (especially Sundays) showing increase in predicted sales, likely due to higher traffic.\n",
    "- **Month**: Seasonal trend is subtle. A noticeable spike in December may reflect holiday season shopping.\n",
    "- **CompetitionDistance**: Shows diminishing returns—nearby competition has a strong negative impact, but effect plateaus beyond ~5km.\n",
    "\n",
    "These plots provide interpretability to model behavior and confirm the intuitive relationships between business drivers and predicted outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00efc88",
   "metadata": {},
   "source": [
    "## SHAP with Tree-Based Surrogate Model\n",
    "\n",
    "To explore SHAP explanations efficiently, we train a tree-based surrogate model (e.g., HistGradientBoostingRegressor) on the same data. SHAP TreeExplainer provides fast, accurate explanations for such models.\n",
    "\n",
    "This surrogate preserves the input structure and allows clearer insights into feature impacts while maintaining interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Prepare raw inputs and target\n",
    "X = to.items[cat_names + cont_names].copy()\n",
    "y = to.items[dep_var]\n",
    "\n",
    "# Encode categoricals and handle missing values\n",
    "model = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "    HistGradientBoostingRegressor()\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Sample 100 rows to explain\n",
    "X_sample = X.sample(n=100, random_state=42)\n",
    "\n",
    "# Explain with SHAP\n",
    "explainer = shap.Explainer(model.named_steps['histgradientboostingregressor'])\n",
    "shap_values = explainer(X_sample)\n",
    "\n",
    "# Plot and save\n",
    "shap.summary_plot(shap_values, X_sample, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../plots/shap_tree_summary.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e817a7",
   "metadata": {},
   "source": [
    "## SHAP Summary Plot Analysis\n",
    "\n",
    "This SHAP summary plot visualizes the impact of each feature on the surrogate model’s output for 100 sampled predictions. Each point represents a row (observation), with color indicating the feature’s value (blue = low, pink = high) and horizontal position showing SHAP value (positive or negative contribution to prediction).\n",
    "\n",
    "### Key Insights:\n",
    "- **Customers** is by far the most influential feature, with higher values consistently increasing predicted sales.\n",
    "- **Assortment** and **StoreType** also show meaningful spread in SHAP values, suggesting their type influences sales significantly.\n",
    "- Features like **Promo** and **CompetitionDistance** have non-negligible but lower impact, varying based on context.\n",
    "- **Temporal features** (e.g., `Day`, `Month`, `Year`) and promotional timing features (e.g., `Promo2SinceYear`, `Promo2SinceWeek`) appear less influential in this surrogate model.\n",
    "\n",
    "### Comparison:\n",
    "- The SHAP plot generally agrees with both permutation and drop-column importances regarding **Customers** being dominant.\n",
    "- However, SHAP gives a richer view by illustrating **directional impact** (positive vs. negative) and the distribution of values.\n",
    "\n",
    "This plot helps verify and interpret model behavior, guiding which features may warrant closer business analysis or cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9045590",
   "metadata": {},
   "source": [
    "## SHAP Directional Impact – Customers\n",
    "\n",
    "The SHAP summary plot shows that `Customers` is the most influential feature in the model's predictions.\n",
    "\n",
    "- **High values of `Customers` (red dots)** are located far to the **right**, meaning they significantly **increase** predicted sales.\n",
    "- **Low values of `Customers` (blue dots)** are located far to the **left**, meaning they substantially **decrease** predicted sales.\n",
    "\n",
    "This is expected: when a store has more customers, the model predicts higher sales. Conversely, fewer customers lead to lower sales predictions.\n",
    "\n",
    "### Summary:\n",
    "- `Customers` has both **high magnitude** and **strong directional impact**.\n",
    "- It serves as a **direct proxy** for `Sales`, which is the prediction target.\n",
    "- The plot confirms the model is heavily relying on this variable for accurate forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40817e",
   "metadata": {},
   "source": [
    "## 4. Visualizing Embedding Weights\n",
    "\n",
    "Neural networks with categorical inputs use embedding layers to convert categories into dense vector representations. These embeddings are learned during training and capture relationships between categories.\n",
    "\n",
    "This section visualizes the learned embeddings for selected categorical variables. By projecting them to 2D using PCA, we can inspect how the model clusters similar categories in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7214c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify key categorical variables to visualize\n",
    "important_cat_vars = ['Store', 'StoreType', 'Assortment', 'DayOfWeek', 'PromoInterval']\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_embedding(var_name, embed_weights, labels):\n",
    "    pca = PCA(n_components=2)\n",
    "    weights_2d = pca.fit_transform(embed_weights)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(weights_2d[:, 0], weights_2d[:, 1])\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.annotate(str(label), (weights_2d[i, 0], weights_2d[i, 1]))\n",
    "\n",
    "    plt.title(f\"PCA Projection of '{var_name}' Embedding\")\n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    os.makedirs(\"../plots\", exist_ok=True)\n",
    "    path = f\"../plots/embedding_{var_name.lower()}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Loop through and plot only selected embeddings\n",
    "for var_name in important_cat_vars:\n",
    "    if var_name in to.cat_names:\n",
    "        i = to.cat_names.index(var_name)\n",
    "        embedding_layer = learn.model.embeds[i]\n",
    "        weights = embedding_layer.weight.data.cpu().numpy()\n",
    "        labels = to.classes[var_name]\n",
    "        plot_embedding(var_name, weights, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0223d4",
   "metadata": {},
   "source": [
    "## 3.6 Visualizing Embedding Weights via PCA\n",
    "\n",
    "To gain interpretability from the learned categorical embeddings in our neural network model, we projected the learned weights for selected categorical variables into 2D using PCA. These embeddings capture compressed representations of categories and their relational structure, learned directly from the sales prediction task.\n",
    "\n",
    "We visualized the following embeddings:\n",
    "\n",
    "- **Store**: Each store is assigned a unique embedding vector. The PCA plot shows broad dispersion and clustering in some regions, suggesting that stores may share latent similarities (e.g., similar size, traffic, or promotion history). However, due to the large number of stores, interpretability is limited without additional metadata.\n",
    "  ![store embedding](../plots/embedding_store.png)\n",
    "\n",
    "- **StoreType**: A much smaller and interpretable embedding. StoreType categories `a`, `b`, `c`, `d`, and missing values are clearly separated, indicating that the model has learned distinct internal representations for different store types.\n",
    "  ![storetype embedding](../plots/embedding_storetype.png)\n",
    "\n",
    "- **Assortment**: Types `a`, `b`, and `c` form distinct clusters. This suggests the model sees them as contributing differently to sales, likely due to product variety.\n",
    "  ![assortment embedding](../plots/embedding_assortment.png)\n",
    "\n",
    "- **DayOfWeek**: Displays a spread of embeddings, showing that the model has learned meaningful distinctions across the week (e.g., lower weekend sales, different weekday patterns).\n",
    "  ![dayofweek embedding](../plots/embedding_dayofweek.png)\n",
    "\n",
    "- **PromoInterval**: Shows how different promotion schedules are internally represented. Clusters suggest the model captures seasonal patterns or calendar alignment effects.\n",
    "  ![promointerval embedding](../plots/embedding_promointerval.png)\n",
    "\n",
    "### Summary\n",
    "\n",
    "These PCA projections give us a window into how the model groups and separates different category levels based on their contribution to predicting sales. This is particularly useful when trying to debug or explain model behavior. It also complements SHAP and permutation/drop-column importance by focusing on **what the model has learned internally**, rather than input-output relationships alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55acc01",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We conducted a comprehensive analysis of model interpretability using several techniques.\n",
    "- **Permutation feature importance** was computed using a validation set and revealed that `Customers`, `Store`, `Open`, and `Promo` were among the most impactful features, aligning well with business intuition.\n",
    "- We compared this to our prior **drop-column feature importance**, which highlighted similar top contributors but included more impact from engineered or missingness-indicator features like `CompetitionOpenSinceMonth_na`.\n",
    "- **Partial dependence plots (PDPs)** were generated for selected features such as `Customers`, `Promo`, `Month`, and `DayOfWeek`, revealing directional relationships between inputs and predicted sales. For example:\n",
    "  - Higher `Customers` counts led to higher sales predictions.\n",
    "  - Active promotions (`Promo = 1`) had a strong positive effect.\n",
    "- We used **SHAP (SHapley Additive exPlanations)** to understand the per-instance contribution of each feature. SHAP summary plots confirmed the dominance of `Customers`, `Assortment`, `StoreType`, and `Promo`.\n",
    "- Finally, we explored the **embedding weights** learned by the neural network for key categorical features using PCA. Embeddings for `StoreType`, `Assortment`, and `PromoInterval` showed clear spatial separation, validating that the model had learned distinct representations for different categories.\n",
    "\n",
    "### Key Deliverables\n",
    "- Bar plots for permutation feature importance and SHAP summary\n",
    "- PDPs for top 5 features\n",
    "- PCA plots for learned categorical embeddings\n",
    "- Markdown commentary explaining interpretability outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-tabular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
